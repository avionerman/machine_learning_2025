{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMWEfaOH8FCybDT682uuxZ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avionerman/machine_learning_2025/blob/main/poverty_competition_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dxS3o-LGDAJj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor\n",
        "except ImportError:\n",
        "    print(\"CatBoost not found, installing...\")\n",
        "    !pip install catboost -qq\n",
        "    from catboost import CatBoostRegressor\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Config:\n",
        "    \"\"\"Central configuration for all parameters.\"\"\"\n",
        "\n",
        "    TRAIN_FEATURES_PATH = '/content/train_hh_features.csv'\n",
        "    TRAIN_LABELS_PATH = '/content/train_hh_gt.csv'\n",
        "    TEST_FEATURES_PATH = '/content/test_hh_features.csv'\n",
        "\n",
        "    # target col\n",
        "    TARGET_COL = 'cons_ppp17'\n",
        "\n",
        "    # ID cols\n",
        "    ID_COLS = ['survey_id', 'hhid']\n",
        "\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "    # validation split\n",
        "    VAL_SIZE = 0.2\n",
        "\n",
        "    # poverty thresholds for competition metric\n",
        "    POVERTY_THRESHOLDS = [\n",
        "        3.17, 3.94, 4.60, 5.26, 5.88, 6.47, 7.06, 7.70,\n",
        "        8.40, 9.13, 9.87, 10.70, 11.62, 12.69, 14.03,\n",
        "        15.64, 17.76, 20.99, 27.37\n",
        "    ]\n",
        "\n",
        "    # percentile ranks for weighting\n",
        "    PERCENTILE_RANKS = [\n",
        "        0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40,\n",
        "        0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80,\n",
        "        0.85, 0.90, 0.95\n",
        "    ]\n",
        "\n",
        "    # survey IDs\n",
        "    TEST_SURVEY_IDS = [400000, 500000, 600000]\n",
        "\n",
        "    # binary mappings\n",
        "    BINARY_MAPPINGS = {\n",
        "        'Yes': 1, 'No': 0,\n",
        "        'Male': 1, 'Female': 0,\n",
        "        'Access': 1, 'No access': 0,\n",
        "        'Urban': 1, 'Rural': 0,\n",
        "        'Owner': 1, 'Renter': 0, 'Not owner': 0,\n",
        "        'Employed': 1, 'Not employed': 0,\n",
        "        1: 1, 0: 0,\n",
        "        1.0: 1, 0.0: 0\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "KKTWr1Q0DDK_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles all data loading, cleaning, and preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load training and test data.\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "\n",
        "        train_features = pd.read_csv(self.config.TRAIN_FEATURES_PATH)\n",
        "        train_labels = pd.read_csv(self.config.TRAIN_LABELS_PATH)\n",
        "        test_features = pd.read_csv(self.config.TEST_FEATURES_PATH)\n",
        "\n",
        "        train = train_features.merge(train_labels, on=self.config.ID_COLS)\n",
        "\n",
        "        print(f\"Training data shape: {train.shape}\")\n",
        "        print(f\"Test data shape: {test_features.shape}\")\n",
        "\n",
        "        return train, test_features\n",
        "\n",
        "    def handle_missing_values(self, train, test):\n",
        "        \"\"\"Fill missing values in both datasets.\"\"\"\n",
        "        print(\"Handling missing values...\")\n",
        "\n",
        "        train['sector1d'] = train['sector1d'].fillna('Not employed')\n",
        "        test['sector1d'] = test['sector1d'].fillna('Not employed')\n",
        "\n",
        "        mode_dweltyp = train['dweltyp'].mode()[0]\n",
        "        train['dweltyp'] = train['dweltyp'].fillna(mode_dweltyp)\n",
        "        test['dweltyp'] = test['dweltyp'].fillna(mode_dweltyp)\n",
        "\n",
        "        median_utl = train['utl_exp_ppp17'].median()\n",
        "        train['utl_exp_ppp17'] = train['utl_exp_ppp17'].fillna(median_utl)\n",
        "        test['utl_exp_ppp17'] = test['utl_exp_ppp17'].fillna(median_utl)\n",
        "\n",
        "        cols_to_fill = ['employed', 'share_secondary', 'educ_max']\n",
        "        for col in cols_to_fill:\n",
        "            if col in train.columns and train[col].isnull().sum() > 0:\n",
        "                mode_val = train[col].mode()[0]\n",
        "                train[col] = train[col].fillna(mode_val)\n",
        "                test[col] = test[col].fillna(mode_val)\n",
        "\n",
        "        consumed_cols = [col for col in train.columns if col.startswith('consumed')]\n",
        "        for col in consumed_cols:\n",
        "            if train[col].isnull().sum() > 0:\n",
        "                mode_val = train[col].mode()[0]\n",
        "                train[col] = train[col].fillna(mode_val)\n",
        "                test[col] = test[col].fillna(mode_val)\n",
        "\n",
        "        print(f\"Missing values after filling: {train.isnull().sum().sum()}\")\n",
        "        return train, test\n",
        "\n",
        "    def encode_binary_columns(self, train, test):\n",
        "        \"\"\"Encode binary columns with robust handling.\"\"\"\n",
        "        print(\"Encoding binary columns...\")\n",
        "\n",
        "        consumed_cols = [col for col in train.columns if col.startswith('consumed')]\n",
        "        binary_cols = ['male', 'owner', 'water', 'toilet', 'sewer', 'elect',\n",
        "                       'employed', 'any_nonagric', 'urban'] + consumed_cols\n",
        "\n",
        "        for col in binary_cols:\n",
        "            if col in train.columns:\n",
        "                unique_train = train[col].unique()\n",
        "                unique_test = test[col].unique()\n",
        "\n",
        "                if train[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
        "                    train[col] = train[col].astype(int)\n",
        "                    test[col] = test[col].astype(int)\n",
        "                else:\n",
        "                    train[col] = train[col].map(self.config.BINARY_MAPPINGS)\n",
        "                    test[col] = test[col].map(self.config.BINARY_MAPPINGS)\n",
        "\n",
        "                    if train[col].isnull().any():\n",
        "                        print(f\"  Warning: Unmapped values in {col} (train). Unique before: {unique_train}\")\n",
        "                        train[col] = train[col].fillna(0)\n",
        "                    if test[col].isnull().any():\n",
        "                        print(f\"  Warning: Unmapped values in {col} (test). Unique before: {unique_test}\")\n",
        "                        test[col] = test[col].fillna(0)\n",
        "\n",
        "                    train[col] = train[col].astype(int)\n",
        "                    test[col] = test[col].astype(int)\n",
        "\n",
        "        print(f\"Binary columns encoded: {len(binary_cols)}\")\n",
        "        return train, test, binary_cols\n",
        "\n",
        "    def encode_multiclass_columns(self, train, test):\n",
        "        \"\"\"Encode multiclass columns using LabelEncoder.\"\"\"\n",
        "        print(\"Encoding multiclass columns...\")\n",
        "\n",
        "        multiclass_cols = ['water_source', 'sanitation_source', 'dweltyp', 'educ_max', 'sector1d']\n",
        "\n",
        "        for col in multiclass_cols:\n",
        "            if col in train.columns:\n",
        "                le = LabelEncoder()\n",
        "                train[col] = train[col].astype(str)\n",
        "                test[col] = test[col].astype(str)\n",
        "\n",
        "                combined = pd.concat([train[col], test[col]], axis=0)\n",
        "                le.fit(combined)\n",
        "                train[col] = le.transform(train[col])\n",
        "                test[col] = le.transform(test[col])\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        print(f\"Multiclass columns encoded: {len(multiclass_cols)}\")\n",
        "        return train, test, multiclass_cols\n",
        "\n",
        "    def define_feature_columns(self, train, binary_cols, multiclass_cols):\n",
        "        \"\"\"Define all feature columns.\"\"\"\n",
        "        print(\"Defining feature columns...\")\n",
        "\n",
        "        numerical_cols = [\n",
        "            'weight', 'utl_exp_ppp17', 'hsize', 'num_children5', 'num_children10',\n",
        "            'num_children18', 'age', 'num_adult_female', 'num_adult_male',\n",
        "            'num_elderly', 'sworkershh', 'share_secondary', 'sfworkershh'\n",
        "        ]\n",
        "\n",
        "        region_cols = ['region1', 'region2', 'region3', 'region4', 'region5', 'region6', 'region7']\n",
        "\n",
        "        feature_cols = binary_cols + multiclass_cols + numerical_cols + region_cols\n",
        "        feature_cols = [col for col in feature_cols if col in train.columns and col != self.config.TARGET_COL]\n",
        "        feature_cols = list(dict.fromkeys(feature_cols))\n",
        "\n",
        "        self.feature_cols = feature_cols\n",
        "        print(f\"Total feature columns: {len(feature_cols)}\")\n",
        "\n",
        "        return feature_cols\n",
        "\n",
        "    def prepare_data(self, train, test, feature_cols):\n",
        "        \"\"\"Prepare feature matrices and target.\"\"\"\n",
        "        print(\"Preparing feature matrices...\")\n",
        "\n",
        "        X = train[feature_cols].copy()\n",
        "        y = train[self.config.TARGET_COL].copy()\n",
        "        X_test = test[feature_cols].copy()\n",
        "\n",
        "        sample_weights = train['weight'].values\n",
        "        survey_ids = train['survey_id'].values\n",
        "\n",
        "        y_log = np.log1p(y)\n",
        "\n",
        "        print(f\"X shape: {X.shape}\")\n",
        "        print(f\"y shape: {y.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "        return X, y_log, X_test, sample_weights, survey_ids\n",
        "\n",
        "    def create_splits(self, X, y_log, sample_weights, survey_ids):\n",
        "        \"\"\"Create train/validation splits with survey IDs.\"\"\"\n",
        "        print(\"Creating train/validation splits...\")\n",
        "\n",
        "        x_train, x_val, y_train, y_val, w_train, w_val, sid_train, sid_val = train_test_split(\n",
        "            X, y_log, sample_weights, survey_ids,\n",
        "            test_size=self.config.VAL_SIZE,\n",
        "            random_state=self.config.RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {x_train.shape[0]} samples\")\n",
        "        print(f\"Validation set: {x_val.shape[0]} samples\")\n",
        "        print(f\"Surveys in validation: {np.unique(sid_val)}\")\n",
        "\n",
        "        return x_train, x_val, y_train, y_val, w_train, w_val, sid_train, sid_val\n",
        "\n",
        "    def scale_features(self, x_train, x_val, X, X_test):\n",
        "        \"\"\"Scale features for neural network.\"\"\"\n",
        "        print(\"Scaling features...\")\n",
        "\n",
        "        x_train_scaled = self.scaler.fit_transform(x_train)\n",
        "        x_val_scaled = self.scaler.transform(x_val)\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        return x_train_scaled, x_val_scaled, X_scaled, X_test_scaled\n",
        "\n",
        "    def process_all(self):\n",
        "        \"\"\"Run the complete data processing pipeline.\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STARTING DATA PROCESSING PIPELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        train, test = self.load_data()\n",
        "        train, test = self.handle_missing_values(train, test)\n",
        "        train, test, binary_cols = self.encode_binary_columns(train, test)\n",
        "        train, test, multiclass_cols = self.encode_multiclass_columns(train, test)\n",
        "        feature_cols = self.define_feature_columns(train, binary_cols, multiclass_cols)\n",
        "        X, y_log, X_test, sample_weights, survey_ids = self.prepare_data(train, test, feature_cols)\n",
        "        x_train, x_val, y_train, y_val, w_train, w_val, sid_train, sid_val = self.create_splits(\n",
        "            X, y_log, sample_weights, survey_ids\n",
        "        )\n",
        "        x_train_scaled, x_val_scaled, X_scaled, X_test_scaled = self.scale_features(x_train, x_val, X, X_test)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"DATA PROCESSING COMPLETE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return {\n",
        "            'train': train,\n",
        "            'test': test,\n",
        "            'X': X,\n",
        "            'y_log': y_log,\n",
        "            'X_test': X_test,\n",
        "            'sample_weights': sample_weights,\n",
        "            'survey_ids': survey_ids,\n",
        "            'x_train': x_train,\n",
        "            'x_val': x_val,\n",
        "            'y_train': y_train,\n",
        "            'y_val': y_val,\n",
        "            'w_train': w_train,\n",
        "            'w_val': w_val,\n",
        "            'sid_train': sid_train,\n",
        "            'sid_val': sid_val,\n",
        "            'x_train_scaled': x_train_scaled,\n",
        "            'x_val_scaled': x_val_scaled,\n",
        "            'X_scaled': X_scaled,\n",
        "            'X_test_scaled': X_test_scaled,\n",
        "            'feature_cols': feature_cols\n",
        "        }\n"
      ],
      "metadata": {
        "id": "raJEQYtZDFCb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"\n",
        "    Calculates the competition metric exactly as defined:\n",
        "    metric = (1/S) × Σs [ (90/Σw) × Σt(wt × |rt - r̂t|/rt) + (10/H) × Σh(|ch - ĉh|/ch) ]\n",
        "\n",
        "    Key: Calculate per survey, then average across surveys.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.threshold_weights = [1 - abs(0.4 - p) for p in config.PERCENTILE_RANKS]\n",
        "        self.sum_weights = sum(self.threshold_weights)\n",
        "\n",
        "    def calculate(self, y_pred_log, y_true_log, survey_ids=None, return_details=False):\n",
        "        \"\"\"\n",
        "        Calculate competition metric for predictions in log scale.\n",
        "        \"\"\"\n",
        "        y_pred = np.expm1(y_pred_log)\n",
        "        y_true = np.expm1(y_true_log)\n",
        "\n",
        "        return self.calculate_from_original_scale(y_pred, y_true, survey_ids, return_details)\n",
        "\n",
        "    def calculate_from_original_scale(self, y_pred, y_true, survey_ids=None, return_details=False):\n",
        "        \"\"\"\n",
        "        Calculate competition metric when predictions are already in original scale.\n",
        "        \"\"\"\n",
        "        if survey_ids is None:\n",
        "            survey_ids = np.zeros(len(y_true), dtype=int)\n",
        "\n",
        "        unique_surveys = np.unique(survey_ids)\n",
        "\n",
        "        survey_scores = []\n",
        "        survey_poverty_mapes = []\n",
        "        survey_consumption_mapes = []\n",
        "\n",
        "        for survey_id in unique_surveys:\n",
        "            mask = survey_ids == survey_id\n",
        "            y_true_survey = y_true[mask]\n",
        "            y_pred_survey = y_pred[mask]\n",
        "\n",
        "            poverty_errors = []\n",
        "            for threshold, tw in zip(self.config.POVERTY_THRESHOLDS, self.threshold_weights):\n",
        "                actual_rate = (y_true_survey < threshold).mean()\n",
        "                pred_rate = (y_pred_survey < threshold).mean()\n",
        "                if actual_rate > 0:\n",
        "                    error = tw * abs(actual_rate - pred_rate) / actual_rate\n",
        "                    poverty_errors.append(error)\n",
        "\n",
        "            poverty_mape = sum(poverty_errors) / self.sum_weights if poverty_errors else 0\n",
        "            consumption_mape = np.mean(np.abs(y_true_survey - y_pred_survey) / y_true_survey)\n",
        "            survey_score = 0.9 * poverty_mape + 0.1 * consumption_mape\n",
        "\n",
        "            survey_scores.append(survey_score)\n",
        "            survey_poverty_mapes.append(poverty_mape)\n",
        "            survey_consumption_mapes.append(consumption_mape)\n",
        "\n",
        "        final_score = np.mean(survey_scores)\n",
        "        avg_poverty_mape = np.mean(survey_poverty_mapes)\n",
        "        avg_consumption_mape = np.mean(survey_consumption_mapes)\n",
        "\n",
        "        if return_details:\n",
        "            return final_score, avg_poverty_mape, avg_consumption_mape\n",
        "        return final_score\n",
        "\n"
      ],
      "metadata": {
        "id": "zattg9ntDHut"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Trains machine learning models: XGBoost, LightGBM, CatBoost, Neural Network.\"\"\"\n",
        "\n",
        "    def __init__(self, config, metric):\n",
        "        self.config = config\n",
        "        self.metric = metric\n",
        "        self.models = {}\n",
        "        self.val_predictions = {}\n",
        "        self.test_predictions = {}\n",
        "\n",
        "    def train_xgboost(self, X, y_log, x_val, y_val, sid_val):\n",
        "        \"\"\"Train XGBoost model.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Training XGBoost...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        model = xgb.XGBRegressor(\n",
        "            n_estimators=300,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.7,\n",
        "            colsample_bytree=0.8,\n",
        "            min_child_weight=3,\n",
        "            random_state=self.config.RANDOM_STATE,\n",
        "            tree_method='hist'\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y_log)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Training time: {train_time:.2f} seconds\")\n",
        "\n",
        "        y_pred_val = model.predict(x_val)\n",
        "        score, pov_mape, cons_mape = self.metric.calculate(\n",
        "            y_pred_val, y_val.values, sid_val, return_details=True\n",
        "        )\n",
        "        print(f\"Validation Score: {score*100:.2f}% (Poverty: {pov_mape*100:.2f}%, Consumption: {cons_mape*100:.2f}%)\")\n",
        "\n",
        "        self.models['XGBoost'] = model\n",
        "        self.val_predictions['XGBoost'] = y_pred_val\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lightgbm(self, X, y_log, x_val, y_val, sid_val):\n",
        "        \"\"\"Train LightGBM model.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Training LightGBM...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        model = lgb.LGBMRegressor(\n",
        "            n_estimators=300,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.7,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=self.config.RANDOM_STATE,\n",
        "            verbose=-1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y_log)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Training time: {train_time:.2f} seconds\")\n",
        "\n",
        "        y_pred_val = model.predict(x_val)\n",
        "        score, pov_mape, cons_mape = self.metric.calculate(\n",
        "            y_pred_val, y_val.values, sid_val, return_details=True\n",
        "        )\n",
        "        print(f\"Validation Score: {score*100:.2f}% (Poverty: {pov_mape*100:.2f}%, Consumption: {cons_mape*100:.2f}%)\")\n",
        "\n",
        "        self.models['LightGBM'] = model\n",
        "        self.val_predictions['LightGBM'] = y_pred_val\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_catboost(self, X, y_log, x_val, y_val, sid_val):\n",
        "        \"\"\"Train CatBoost model on GPU.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Training CatBoost (GPU)...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=500,\n",
        "            depth=8,\n",
        "            learning_rate=0.05,\n",
        "            random_seed=self.config.RANDOM_STATE,\n",
        "            verbose=50,\n",
        "            task_type='GPU'\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y_log)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Training time: {train_time:.2f} seconds\")\n",
        "\n",
        "        y_pred_val = model.predict(x_val)\n",
        "        score, pov_mape, cons_mape = self.metric.calculate(\n",
        "            y_pred_val, y_val.values, sid_val, return_details=True\n",
        "        )\n",
        "        print(f\"Validation Score: {score*100:.2f}% (Poverty: {pov_mape*100:.2f}%, Consumption: {cons_mape*100:.2f}%)\")\n",
        "\n",
        "        self.models['CatBoost'] = model\n",
        "        self.val_predictions['CatBoost'] = y_pred_val\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_neural_network(self, X_scaled, y_log, x_val_scaled, y_val, sid_val):\n",
        "        \"\"\"Train Neural Network model.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Training Neural Network...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(128, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        early_stop = keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(\n",
        "            X_scaled, y_log,\n",
        "            validation_split=0.2,\n",
        "            epochs=100,\n",
        "            batch_size=256,\n",
        "            callbacks=[early_stop],\n",
        "            verbose=1\n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Training time: {train_time:.2f} seconds\")\n",
        "\n",
        "        y_pred_val = model.predict(x_val_scaled, verbose=0).flatten()\n",
        "        score, pov_mape, cons_mape = self.metric.calculate(\n",
        "            y_pred_val, y_val.values, sid_val, return_details=True\n",
        "        )\n",
        "        print(f\"Validation Score: {score*100:.2f}% (Poverty: {pov_mape*100:.2f}%, Consumption: {cons_mape*100:.2f}%)\")\n",
        "\n",
        "        self.models['NeuralNet'] = model\n",
        "        self.val_predictions['NeuralNet'] = y_pred_val\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_all(self, data):\n",
        "        \"\"\"Train all models.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TRAINING ALL MODELS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        self.train_xgboost(\n",
        "            data['X'], data['y_log'],\n",
        "            data['x_val'], data['y_val'], data['sid_val']\n",
        "        )\n",
        "\n",
        "        self.train_lightgbm(\n",
        "            data['X'], data['y_log'],\n",
        "            data['x_val'], data['y_val'], data['sid_val']\n",
        "        )\n",
        "\n",
        "        self.train_catboost(\n",
        "            data['X'], data['y_log'],\n",
        "            data['x_val'], data['y_val'], data['sid_val']\n",
        "        )\n",
        "\n",
        "        self.train_neural_network(\n",
        "            data['X_scaled'], data['y_log'],\n",
        "            data['x_val_scaled'], data['y_val'], data['sid_val']\n",
        "        )\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ALL MODELS TRAINED\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return self.models, self.val_predictions\n",
        "\n",
        "    def generate_test_predictions(self, X_test, X_test_scaled):\n",
        "        \"\"\"Generate predictions on test set for all models.\"\"\"\n",
        "        print(\"\\nGenerating test predictions for all models...\")\n",
        "\n",
        "        self.test_predictions['XGBoost'] = self.models['XGBoost'].predict(X_test)\n",
        "        self.test_predictions['LightGBM'] = self.models['LightGBM'].predict(X_test)\n",
        "        self.test_predictions['CatBoost'] = self.models['CatBoost'].predict(X_test)\n",
        "        self.test_predictions['NeuralNet'] = self.models['NeuralNet'].predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "        print(\"Test predictions generated for all models\")\n",
        "\n",
        "        return self.test_predictions\n"
      ],
      "metadata": {
        "id": "Nv3snvdADM-K"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EnsembleOptimizer:\n",
        "    \"\"\"Optimizes blend weights for competition metric.\"\"\"\n",
        "\n",
        "    def __init__(self, config, metric):\n",
        "        self.config = config\n",
        "        self.metric = metric\n",
        "        self.optimal_weights = None\n",
        "        self.model_names = None\n",
        "        self.best_single_model = None\n",
        "        self.best_single_score = None\n",
        "\n",
        "    def optimize_weights(self, val_predictions, y_val, sid_val):\n",
        "        \"\"\"Find optimal blend weights using competition metric.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"OPTIMIZING BLEND WEIGHTS FOR COMPETITION METRIC\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        self.model_names = list(val_predictions.keys())\n",
        "        predictions_list = [val_predictions[name] for name in self.model_names]\n",
        "\n",
        "        print(\"\\nIndividual Model Scores (per survey metric):\")\n",
        "        print(\"-\" * 40)\n",
        "        self.best_single_score = float('inf')\n",
        "        best_single_idx = 0\n",
        "\n",
        "        for i, (name, pred) in enumerate(zip(self.model_names, predictions_list)):\n",
        "            score = self.metric.calculate(pred, y_val.values, sid_val)\n",
        "            print(f\"  {name}: {score*100:.2f}%\")\n",
        "            if score < self.best_single_score:\n",
        "                self.best_single_score = score\n",
        "                best_single_idx = i\n",
        "                self.best_single_model = name\n",
        "\n",
        "        print(f\"\\nBest single model: {self.best_single_model} ({self.best_single_score*100:.2f}%)\")\n",
        "\n",
        "        def objective(w):\n",
        "            weights = np.array(w) / np.sum(w)\n",
        "            blended = np.zeros_like(predictions_list[0])\n",
        "            for weight, pred in zip(weights, predictions_list):\n",
        "                blended += weight * pred\n",
        "            return self.metric.calculate(blended, y_val.values, sid_val)\n",
        "\n",
        "        n_models = len(predictions_list)\n",
        "        best_result = None\n",
        "        best_score = float('inf')\n",
        "\n",
        "        starting_points = [\n",
        "            np.ones(n_models) / n_models,\n",
        "            np.eye(n_models)[best_single_idx],\n",
        "            np.array([0.6, 0.2, 0.1, 0.1][:n_models]),\n",
        "            np.array([0.4, 0.3, 0.2, 0.1][:n_models]),\n",
        "            np.array([0.5, 0.3, 0.2, 0.0][:n_models]),\n",
        "        ]\n",
        "\n",
        "        for start in starting_points:\n",
        "            if len(start) != n_models:\n",
        "                start = np.ones(n_models) / n_models\n",
        "\n",
        "            result = minimize(\n",
        "                objective,\n",
        "                start,\n",
        "                method='SLSQP',\n",
        "                bounds=[(0, 1)] * n_models,\n",
        "                options={'maxiter': 1000}\n",
        "            )\n",
        "\n",
        "            if result.fun < best_score:\n",
        "                best_score = result.fun\n",
        "                best_result = result\n",
        "\n",
        "        self.optimal_weights = best_result.x / best_result.x.sum()\n",
        "\n",
        "        print(\"\\nOPTIMAL BLEND WEIGHTS:\")\n",
        "        print(\"-\" * 40)\n",
        "        for name, w in zip(self.model_names, self.optimal_weights):\n",
        "            if w > 0.01:\n",
        "                print(f\"  {name}: {w*100:.1f}%\")\n",
        "\n",
        "        blended_pred = np.zeros_like(predictions_list[0])\n",
        "        for name, w in zip(self.model_names, self.optimal_weights):\n",
        "            blended_pred += w * val_predictions[name]\n",
        "\n",
        "        blended_score, pov_mape, cons_mape = self.metric.calculate(\n",
        "            blended_pred, y_val.values, sid_val, return_details=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nBLENDED VALIDATION SCORE: {blended_score*100:.2f}%\")\n",
        "        print(f\"  Poverty MAPE:    {pov_mape*100:.2f}%\")\n",
        "        print(f\"  Consumption MAPE: {cons_mape*100:.2f}%\")\n",
        "\n",
        "        if blended_score > self.best_single_score:\n",
        "            print(f\"\\n WARNING: Blend ({blended_score*100:.2f}%) is worse than {self.best_single_model} ({self.best_single_score*100:.2f}%)\")\n",
        "            print(f\"    Using {self.best_single_model} alone instead!\")\n",
        "            self.optimal_weights = np.zeros(n_models)\n",
        "            self.optimal_weights[best_single_idx] = 1.0\n",
        "\n",
        "        return self.optimal_weights\n",
        "\n",
        "    def blend_predictions(self, test_predictions):\n",
        "        \"\"\"Blend test predictions using optimal weights.\"\"\"\n",
        "        print(\"\\nBlending test predictions...\")\n",
        "\n",
        "        blended_log = np.zeros_like(test_predictions[self.model_names[0]])\n",
        "        for name, w in zip(self.model_names, self.optimal_weights):\n",
        "            blended_log += w * test_predictions[name]\n",
        "\n",
        "        return blended_log\n"
      ],
      "metadata": {
        "id": "vdavTdJYDQYK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PredictionCalibrator:\n",
        "    \"\"\"\n",
        "    Post-processing calibration to directly optimize the competition metric.\n",
        "\n",
        "    Now supports per survey calibration: learns separate (scale, shift) parameters\n",
        "    for each survey, which can better capture survey-specific characteristics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, metric):\n",
        "        self.config = config\n",
        "        self.metric = metric\n",
        "        self.global_params = None\n",
        "        self.per_survey_params = {}\n",
        "        self.best_score = None\n",
        "        self.score_before = None\n",
        "        self.use_per_survey = True\n",
        "\n",
        "    def _optimize_params(self, y_pred_orig, y_true_orig, survey_ids=None):\n",
        "        \"\"\"Find optimal (scale, shift) for given predictions.\"\"\"\n",
        "\n",
        "        def objective(params):\n",
        "            scale, shift = params\n",
        "            y_calibrated = scale * y_pred_orig + shift\n",
        "            y_calibrated = np.maximum(y_calibrated, 0.01)\n",
        "            return self.metric.calculate_from_original_scale(\n",
        "                y_calibrated, y_true_orig, survey_ids\n",
        "            )\n",
        "\n",
        "        best_result = None\n",
        "        best_score = float('inf')\n",
        "\n",
        "        starting_points = [\n",
        "            [1.0, 0.0],\n",
        "            [1.0, 0.5],\n",
        "            [1.0, -0.5],\n",
        "            [1.05, 0.0],\n",
        "            [0.95, 0.0],\n",
        "            [1.02, 0.2],\n",
        "            [0.98, -0.2],\n",
        "            [1.1, 0.0],\n",
        "            [0.9, 0.0],\n",
        "            [1.0, 1.0],\n",
        "            [1.0, -1.0],\n",
        "            [1.1, -0.5],\n",
        "            [0.9, 0.5],\n",
        "            [1.15, -1.0],\n",
        "            [0.85, 1.0],\n",
        "        ]\n",
        "\n",
        "        for start in starting_points:\n",
        "            result = minimize(\n",
        "                objective,\n",
        "                start,\n",
        "                method='Nelder-Mead',\n",
        "                options={'maxiter': 2000, 'xatol': 1e-8, 'fatol': 1e-8}\n",
        "            )\n",
        "\n",
        "            if result.fun < best_score:\n",
        "                best_score = result.fun\n",
        "                best_result = result\n",
        "\n",
        "        # try bounded optimization\n",
        "        result_bounded = minimize(\n",
        "            objective,\n",
        "            [1.0, 0.0],\n",
        "            method='L-BFGS-B',\n",
        "            bounds=[(0.5, 1.5), (-5.0, 5.0)],\n",
        "            options={'maxiter': 2000}\n",
        "        )\n",
        "\n",
        "        if result_bounded.fun < best_score:\n",
        "            best_score = result_bounded.fun\n",
        "            best_result = result_bounded\n",
        "\n",
        "        return best_result.x, best_score\n",
        "\n",
        "    def calibrate(self, y_pred_log, y_true_log, survey_ids):\n",
        "        \"\"\"\n",
        "        Find optimal calibration parameters using per survey calibration.\n",
        "\n",
        "        Learns separate (scale, shift) for each survey in the validation set.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"CALIBRATING PREDICTIONS (Per survey)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # back to original scale\n",
        "        y_pred_orig = np.expm1(y_pred_log)\n",
        "        y_true_orig = np.expm1(y_true_log)\n",
        "\n",
        "        # scoring before calibration\n",
        "        self.score_before = self.metric.calculate_from_original_scale(\n",
        "            y_pred_orig, y_true_orig, survey_ids\n",
        "        )\n",
        "        print(f\"\\nScore Before calibration: {self.score_before*100:.2f}%\")\n",
        "\n",
        "        # global calibration\n",
        "        print(\"\\n--- Global calibration ---\")\n",
        "        self.global_params, global_score = self._optimize_params(\n",
        "            y_pred_orig, y_true_orig, survey_ids\n",
        "        )\n",
        "        print(f\"Global params: scale={self.global_params[0]:.4f}, shift={self.global_params[1]:.4f}\")\n",
        "        print(f\"Global calibration score: {global_score*100:.2f}%\")\n",
        "\n",
        "        # per survey calibration\n",
        "        print(\"\\n--- Per survey calibration ---\")\n",
        "        unique_surveys = np.unique(survey_ids)\n",
        "\n",
        "        y_calibrated_per_survey = np.zeros_like(y_pred_orig)\n",
        "\n",
        "        for survey_id in unique_surveys:\n",
        "            mask = survey_ids == survey_id\n",
        "            y_pred_survey = y_pred_orig[mask]\n",
        "            y_true_survey = y_true_orig[mask]\n",
        "\n",
        "            # optimization for this survey\n",
        "            params, score = self._optimize_params(y_pred_survey, y_true_survey, None)\n",
        "            self.per_survey_params[survey_id] = params\n",
        "\n",
        "            # applying calibration\n",
        "            scale, shift = params\n",
        "            y_calibrated_per_survey[mask] = scale * y_pred_survey + shift\n",
        "\n",
        "            print(f\"  Survey {survey_id}: scale={scale:.4f}, shift={shift:.4f}, score={score*100:.2f}%\")\n",
        "\n",
        "        # ensuring no negative values\n",
        "        y_calibrated_per_survey = np.maximum(y_calibrated_per_survey, 0.01)\n",
        "\n",
        "        # calculating per survey calibration score\n",
        "        per_survey_score = self.metric.calculate_from_original_scale(\n",
        "            y_calibrated_per_survey, y_true_orig, survey_ids\n",
        "        )\n",
        "\n",
        "        print(f\"\\nPer survey calibration score: {per_survey_score*100:.2f}%\")\n",
        "\n",
        "        # deciding which one to use\n",
        "        if per_survey_score < global_score:\n",
        "            print(f\"\\n✓ Using per survey calibration (better by {(global_score - per_survey_score)*100:.2f}%)\")\n",
        "            self.use_per_survey = True\n",
        "            self.best_score = per_survey_score\n",
        "            y_calibrated_orig = y_calibrated_per_survey\n",
        "        else:\n",
        "            print(f\"\\n✓ Using global calibration (better by {(per_survey_score - global_score)*100:.2f}%)\")\n",
        "            self.use_per_survey = False\n",
        "            self.best_score = global_score\n",
        "            scale, shift = self.global_params\n",
        "            y_calibrated_orig = scale * y_pred_orig + shift\n",
        "            y_calibrated_orig = np.maximum(y_calibrated_orig, 0.01)\n",
        "\n",
        "        improvement = self.score_before - self.best_score\n",
        "        print(f\"\\nFinal improvement: {improvement*100:.2f} percentage points\")\n",
        "        print(f\"Score: {self.score_before*100:.2f}% → {self.best_score*100:.2f}%\")\n",
        "\n",
        "        # Convert back to log scale\n",
        "        y_calibrated_log = np.log1p(y_calibrated_orig)\n",
        "\n",
        "        return y_calibrated_log\n",
        "\n",
        "    def apply_calibration(self, y_pred_log, test_survey_ids):\n",
        "        \"\"\"\n",
        "        Apply the learned calibration parameters to test predictions.\n",
        "\n",
        "        For per survey calibration, maps training surveys to test surveys:\n",
        "        - Training: 100000, 200000, 300000\n",
        "        - Test: 400000, 500000, 600000\n",
        "\n",
        "        Mapping strategy: Use average of all training survey parameters,\n",
        "        or map by survey order (100000->400000, 200000->500000, 300000->600000)\n",
        "        \"\"\"\n",
        "        if self.global_params is None:\n",
        "            raise ValueError(\"Must call calibrate() first to learn parameters\")\n",
        "\n",
        "        y_pred_orig = np.expm1(y_pred_log)\n",
        "\n",
        "        if not self.use_per_survey:\n",
        "            # using global calibration\n",
        "            scale, shift = self.global_params\n",
        "            y_calibrated_orig = scale * y_pred_orig + shift\n",
        "        else:\n",
        "            # using per survey calibration\n",
        "            # Map training surveys to test surveys\n",
        "            survey_mapping = {\n",
        "                400000: 100000,  # Test survey 400000 uses params from train survey 100000\n",
        "                500000: 200000,  # Test survey 500000 uses params from train survey 200000\n",
        "                600000: 300000,  # Test survey 600000 uses params from train survey 300000\n",
        "            }\n",
        "\n",
        "            y_calibrated_orig = np.zeros_like(y_pred_orig)\n",
        "\n",
        "            for test_sid in [400000, 500000, 600000]:\n",
        "                mask = test_survey_ids == test_sid\n",
        "                if mask.sum() == 0:\n",
        "                    continue\n",
        "\n",
        "                train_sid = survey_mapping[test_sid]\n",
        "\n",
        "                if train_sid in self.per_survey_params:\n",
        "                    scale, shift = self.per_survey_params[train_sid]\n",
        "                else:\n",
        "                    # falling back to global params\n",
        "                    scale, shift = self.global_params\n",
        "\n",
        "                y_calibrated_orig[mask] = scale * y_pred_orig[mask] + shift\n",
        "\n",
        "            print(f\"\\nApplied per survey calibration:\")\n",
        "            for test_sid, train_sid in survey_mapping.items():\n",
        "                if train_sid in self.per_survey_params:\n",
        "                    scale, shift = self.per_survey_params[train_sid]\n",
        "                    print(f\"  Survey {test_sid} (using {train_sid} params): scale={scale:.4f}, shift={shift:.4f}\")\n",
        "\n",
        "        y_calibrated_orig = np.maximum(y_calibrated_orig, 0.01)\n",
        "        y_calibrated_log = np.log1p(y_calibrated_orig)\n",
        "\n",
        "        return y_calibrated_log\n"
      ],
      "metadata": {
        "id": "OTmi-nQpDUY9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SubmissionGenerator:\n",
        "    \"\"\"Generates submission files for DrivenData competition.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def generate(self, test_features, y_test_pred):\n",
        "        \"\"\"Generate submission files.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"GENERATING SUBMISSION FILES\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        submission_hh = pd.DataFrame({\n",
        "            'survey_id': test_features['survey_id'],\n",
        "            'hhid': test_features['hhid'],\n",
        "            'cons_ppp17': y_test_pred\n",
        "        })\n",
        "\n",
        "        poverty_rates = []\n",
        "        for survey_id in self.config.TEST_SURVEY_IDS:\n",
        "            survey_preds = submission_hh[submission_hh['survey_id'] == survey_id]['cons_ppp17']\n",
        "            row = {'survey_id': survey_id}\n",
        "            for threshold in self.config.POVERTY_THRESHOLDS:\n",
        "                col_name = f'pct_hh_below_{threshold:.2f}'\n",
        "                pct_below = (survey_preds < threshold).mean()\n",
        "                row[col_name] = pct_below\n",
        "            poverty_rates.append(row)\n",
        "\n",
        "        submission_rates = pd.DataFrame(poverty_rates)\n",
        "\n",
        "        submission_hh.to_csv('predicted_household_consumption.csv', index=False)\n",
        "        submission_rates.to_csv('predicted_poverty_distribution.csv', index=False)\n",
        "\n",
        "        with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
        "            zipf.write('predicted_household_consumption.csv')\n",
        "            zipf.write('predicted_poverty_distribution.csv')\n",
        "\n",
        "        print(\"\\nFiles created:\")\n",
        "        print(f\"  1. predicted_household_consumption.csv - Shape: {submission_hh.shape}\")\n",
        "        print(f\"  2. predicted_poverty_distribution.csv - Shape: {submission_rates.shape}\")\n",
        "        print(f\"  3. submission.zip\")\n",
        "\n",
        "        print(f\"\\nPrediction Statistics:\")\n",
        "        print(f\"  Mean:   ${y_test_pred.mean():.2f}/day\")\n",
        "        print(f\"  Median: ${np.median(y_test_pred):.2f}/day\")\n",
        "        print(f\"  Min:    ${y_test_pred.min():.2f}/day\")\n",
        "        print(f\"  Max:    ${y_test_pred.max():.2f}/day\")\n",
        "\n",
        "        print(f\"\\nPredicted Poverty Rates (at $5.26/day threshold):\")\n",
        "        for _, row in submission_rates.iterrows():\n",
        "            print(f\"  Survey {int(row['survey_id'])}: {row['pct_hh_below_5.26']*100:.1f}%\")\n",
        "\n",
        "        print(\"\\n✓ Ready to submit to DrivenData!\")\n",
        "\n",
        "        return submission_hh, submission_rates\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the submission zip file.\"\"\"\n",
        "        from google.colab import files\n",
        "        files.download('submission.zip')\n",
        "        print(\"Download started!\")\n"
      ],
      "metadata": {
        "id": "nw_-FwM0DWQk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main pipeline:\n",
        "    1. Process data\n",
        "    2. Train all models (XGBoost, LightGBM, CatBoost, Neural Network)\n",
        "    3. Optimize ensemble weights\n",
        "    4. Calibrate predictions with per survey calibration\n",
        "    5. Generate submission\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"POVERTY PREDICTION PIPELINE\")\n",
        "    print(\"With Ensemble + Per survey Calibration\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Initializing configuration\n",
        "    print(\"\\n[Step 1/7] Initializing configuration...\")\n",
        "    config = Config()\n",
        "\n",
        "    # Step 2: Processing data\n",
        "    print(\"\\n[Step 2/7] Processing data...\")\n",
        "    processor = DataProcessor(config)\n",
        "    data = processor.process_all()\n",
        "\n",
        "    # Step 3: Initializing metric calculator\n",
        "    print(\"\\n[Step 3/7] Initializing competition metric...\")\n",
        "    metric = CompetitionMetric(config)\n",
        "\n",
        "    # Step 4: Training all models\n",
        "    print(\"\\n[Step 4/7] Training models...\")\n",
        "    trainer = ModelTrainer(config, metric)\n",
        "    models, val_predictions = trainer.train_all(data)\n",
        "\n",
        "    # Step 5: Optimizing ensemble weights\n",
        "    print(\"\\n[Step 5/7] Optimizing ensemble...\")\n",
        "    optimizer = EnsembleOptimizer(config, metric)\n",
        "    optimal_weights = optimizer.optimize_weights(\n",
        "        val_predictions, data['y_val'], data['sid_val']\n",
        "    )\n",
        "\n",
        "    # getting blended validation predictions\n",
        "    blended_val_pred = np.zeros_like(val_predictions['XGBoost'])\n",
        "    for name, w in zip(optimizer.model_names, optimal_weights):\n",
        "        blended_val_pred += w * val_predictions[name]\n",
        "\n",
        "    # Step 6: Calibrating predictions (per survey)\n",
        "    print(\"\\n[Step 6/7] Calibrating predictions (per survey)...\")\n",
        "    calibrator = PredictionCalibrator(config, metric)\n",
        "    calibrated_val_pred = calibrator.calibrate(\n",
        "        blended_val_pred,\n",
        "        data['y_val'].values,\n",
        "        data['sid_val']\n",
        "    )\n",
        "\n",
        "    # generating test predictions\n",
        "    test_predictions = trainer.generate_test_predictions(data['X_test'], data['X_test_scaled'])\n",
        "\n",
        "    # blending test predictions\n",
        "    blended_test_pred = optimizer.blend_predictions(test_predictions)\n",
        "\n",
        "    # getting test survey IDs for per survey calibration\n",
        "    test_survey_ids = data['test']['survey_id'].values\n",
        "\n",
        "    # applying calibration to test predictions (with survey IDs)\n",
        "    calibrated_test_pred = calibrator.apply_calibration(blended_test_pred, test_survey_ids)\n",
        "\n",
        "    # converting to original scale\n",
        "    y_test_pred = np.expm1(calibrated_test_pred)\n",
        "\n",
        "    print(f\"\\nFinal Test Predictions (after per survey calibration):\")\n",
        "    print(f\"  Mean:   ${y_test_pred.mean():.2f}/day\")\n",
        "    print(f\"  Median: ${np.median(y_test_pred):.2f}/day\")\n",
        "    print(f\"  Min:    ${y_test_pred.min():.2f}/day\")\n",
        "    print(f\"  Max:    ${y_test_pred.max():.2f}/day\")\n",
        "\n",
        "    # per survey statistics\n",
        "    print(f\"\\nPer survey prediction statistics:\")\n",
        "    for sid in [400000, 500000, 600000]:\n",
        "        mask = test_survey_ids == sid\n",
        "        print(f\"  Survey {sid}: Mean=${y_test_pred[mask].mean():.2f}, Median=${np.median(y_test_pred[mask]):.2f}\")\n",
        "\n",
        "    # Step 7: Generating submission\n",
        "    print(\"\\n[Step 7/7] Generating submission...\")\n",
        "    submission = SubmissionGenerator(config)\n",
        "    submission_hh, submission_rates = submission.generate(data['test'], y_test_pred)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Best single model: {optimizer.best_single_model} ({optimizer.best_single_score*100:.2f}%)\")\n",
        "    print(f\"Ensemble weights: \", end=\"\")\n",
        "    for name, w in zip(optimizer.model_names, optimal_weights):\n",
        "        if w > 0.01:\n",
        "            print(f\"{name}={w*100:.1f}% \", end=\"\")\n",
        "    print()\n",
        "\n",
        "    if calibrator.use_per_survey:\n",
        "        print(f\"Calibration type: Per survey\")\n",
        "        for train_sid, params in calibrator.per_survey_params.items():\n",
        "            print(f\"  Survey {train_sid}: scale={params[0]:.4f}, shift={params[1]:.4f}\")\n",
        "    else:\n",
        "        print(f\"Calibration type: GLOBAL\")\n",
        "        print(f\"  scale={calibrator.global_params[0]:.4f}, shift={calibrator.global_params[1]:.4f}\")\n",
        "\n",
        "    print(f\"\\nScore before calibration: {calibrator.score_before*100:.2f}%\")\n",
        "    print(f\"Score after calibration: {calibrator.best_score*100:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'data': data,\n",
        "        'metric': metric,\n",
        "        'trainer': trainer,\n",
        "        'optimizer': optimizer,\n",
        "        'calibrator': calibrator,\n",
        "        'submission': submission,\n",
        "        'y_test_pred': y_test_pred\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5eTvrApQDYIT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results = main()\n",
        "results['submission'].download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1oa9a55rDaGo",
        "outputId": "ed76f389-42d4-4cfb-f963-5dce969db2c1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "POVERTY PREDICTION PIPELINE\n",
            "With Ensemble + Per survey Calibration\n",
            "============================================================\n",
            "\n",
            "[Step 1/7] Initializing configuration...\n",
            "\n",
            "[Step 2/7] Processing data...\n",
            "============================================================\n",
            "STARTING DATA PROCESSING PIPELINE\n",
            "============================================================\n",
            "Loading data...\n",
            "Training data shape: (104234, 89)\n",
            "Test data shape: (103023, 88)\n",
            "Handling missing values...\n",
            "Missing values after filling: 0\n",
            "Encoding binary columns...\n",
            "Binary columns encoded: 59\n",
            "Encoding multiclass columns...\n",
            "Multiclass columns encoded: 5\n",
            "Defining feature columns...\n",
            "Total feature columns: 84\n",
            "Preparing feature matrices...\n",
            "X shape: (104234, 84)\n",
            "y shape: (104234,)\n",
            "X_test shape: (103023, 84)\n",
            "Creating train/validation splits...\n",
            "Training set: 83387 samples\n",
            "Validation set: 20847 samples\n",
            "Surveys in validation: [100000 200000 300000]\n",
            "Scaling features...\n",
            "\n",
            "============================================================\n",
            "DATA PROCESSING COMPLETE\n",
            "============================================================\n",
            "\n",
            "[Step 3/7] Initializing competition metric...\n",
            "\n",
            "[Step 4/7] Training models...\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL MODELS\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "Training XGBoost...\n",
            "==================================================\n",
            "Training time: 7.54 seconds\n",
            "Validation Score: 8.70% (Poverty: 7.08%, Consumption: 23.28%)\n",
            "\n",
            "==================================================\n",
            "Training LightGBM...\n",
            "==================================================\n",
            "Training time: 1.55 seconds\n",
            "Validation Score: 9.91% (Poverty: 7.98%, Consumption: 27.29%)\n",
            "\n",
            "==================================================\n",
            "Training CatBoost (GPU)...\n",
            "==================================================\n",
            "0:\tlearn: 0.6066610\ttotal: 4.58ms\tremaining: 2.28s\n",
            "50:\tlearn: 0.3472507\ttotal: 181ms\tremaining: 1.6s\n",
            "100:\tlearn: 0.3218869\ttotal: 345ms\tremaining: 1.36s\n",
            "150:\tlearn: 0.3128692\ttotal: 508ms\tremaining: 1.17s\n",
            "200:\tlearn: 0.3074232\ttotal: 671ms\tremaining: 998ms\n",
            "250:\tlearn: 0.3035136\ttotal: 833ms\tremaining: 826ms\n",
            "300:\tlearn: 0.3004619\ttotal: 996ms\tremaining: 659ms\n",
            "350:\tlearn: 0.2977766\ttotal: 1.16s\tremaining: 491ms\n",
            "400:\tlearn: 0.2954396\ttotal: 1.32s\tremaining: 326ms\n",
            "450:\tlearn: 0.2933313\ttotal: 1.48s\tremaining: 161ms\n",
            "499:\tlearn: 0.2913835\ttotal: 1.64s\tremaining: 0us\n",
            "Training time: 1.87 seconds\n",
            "Validation Score: 9.89% (Poverty: 8.04%, Consumption: 26.52%)\n",
            "\n",
            "==================================================\n",
            "Training Neural Network...\n",
            "==================================================\n",
            "Epoch 1/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 4.3730 - mae: 1.7793 - val_loss: 0.2632 - val_mae: 0.3990\n",
            "Epoch 2/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6036 - mae: 0.5956 - val_loss: 0.1536 - val_mae: 0.3036\n",
            "Epoch 3/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3546 - mae: 0.4553 - val_loss: 0.1368 - val_mae: 0.2838\n",
            "Epoch 4/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2572 - mae: 0.3903 - val_loss: 0.1249 - val_mae: 0.2726\n",
            "Epoch 5/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2175 - mae: 0.3603 - val_loss: 0.1223 - val_mae: 0.2689\n",
            "Epoch 6/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1946 - mae: 0.3418 - val_loss: 0.1195 - val_mae: 0.2662\n",
            "Epoch 7/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1770 - mae: 0.3269 - val_loss: 0.1159 - val_mae: 0.2622\n",
            "Epoch 8/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1691 - mae: 0.3195 - val_loss: 0.1143 - val_mae: 0.2610\n",
            "Epoch 9/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1617 - mae: 0.3134 - val_loss: 0.1154 - val_mae: 0.2616\n",
            "Epoch 10/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1546 - mae: 0.3060 - val_loss: 0.1125 - val_mae: 0.2592\n",
            "Epoch 11/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1519 - mae: 0.3021 - val_loss: 0.1113 - val_mae: 0.2581\n",
            "Epoch 12/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1464 - mae: 0.2972 - val_loss: 0.1126 - val_mae: 0.2584\n",
            "Epoch 13/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1453 - mae: 0.2960 - val_loss: 0.1112 - val_mae: 0.2574\n",
            "Epoch 14/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1405 - mae: 0.2900 - val_loss: 0.1093 - val_mae: 0.2559\n",
            "Epoch 15/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1368 - mae: 0.2871 - val_loss: 0.1093 - val_mae: 0.2554\n",
            "Epoch 16/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1356 - mae: 0.2852 - val_loss: 0.1090 - val_mae: 0.2549\n",
            "Epoch 17/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1318 - mae: 0.2820 - val_loss: 0.1080 - val_mae: 0.2549\n",
            "Epoch 18/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1292 - mae: 0.2778 - val_loss: 0.1081 - val_mae: 0.2540\n",
            "Epoch 19/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1289 - mae: 0.2772 - val_loss: 0.1087 - val_mae: 0.2547\n",
            "Epoch 20/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1266 - mae: 0.2754 - val_loss: 0.1073 - val_mae: 0.2539\n",
            "Epoch 21/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1244 - mae: 0.2730 - val_loss: 0.1087 - val_mae: 0.2541\n",
            "Epoch 22/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1227 - mae: 0.2705 - val_loss: 0.1113 - val_mae: 0.2564\n",
            "Epoch 23/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1205 - mae: 0.2683 - val_loss: 0.1078 - val_mae: 0.2538\n",
            "Epoch 24/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1189 - mae: 0.2666 - val_loss: 0.1063 - val_mae: 0.2522\n",
            "Epoch 25/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1178 - mae: 0.2653 - val_loss: 0.1064 - val_mae: 0.2521\n",
            "Epoch 26/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1167 - mae: 0.2635 - val_loss: 0.1062 - val_mae: 0.2523\n",
            "Epoch 27/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1135 - mae: 0.2609 - val_loss: 0.1067 - val_mae: 0.2521\n",
            "Epoch 28/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1139 - mae: 0.2604 - val_loss: 0.1058 - val_mae: 0.2518\n",
            "Epoch 29/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1138 - mae: 0.2604 - val_loss: 0.1060 - val_mae: 0.2523\n",
            "Epoch 30/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1128 - mae: 0.2590 - val_loss: 0.1063 - val_mae: 0.2534\n",
            "Epoch 31/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1101 - mae: 0.2559 - val_loss: 0.1053 - val_mae: 0.2516\n",
            "Epoch 32/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1091 - mae: 0.2556 - val_loss: 0.1068 - val_mae: 0.2523\n",
            "Epoch 33/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1079 - mae: 0.2537 - val_loss: 0.1051 - val_mae: 0.2516\n",
            "Epoch 34/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1080 - mae: 0.2537 - val_loss: 0.1054 - val_mae: 0.2522\n",
            "Epoch 35/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1075 - mae: 0.2530 - val_loss: 0.1057 - val_mae: 0.2515\n",
            "Epoch 36/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1060 - mae: 0.2517 - val_loss: 0.1054 - val_mae: 0.2519\n",
            "Epoch 37/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1049 - mae: 0.2506 - val_loss: 0.1074 - val_mae: 0.2526\n",
            "Epoch 38/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1061 - mae: 0.2516 - val_loss: 0.1054 - val_mae: 0.2513\n",
            "Epoch 39/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1045 - mae: 0.2497 - val_loss: 0.1058 - val_mae: 0.2510\n",
            "Epoch 40/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1049 - mae: 0.2500 - val_loss: 0.1048 - val_mae: 0.2504\n",
            "Epoch 41/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1047 - mae: 0.2500 - val_loss: 0.1059 - val_mae: 0.2515\n",
            "Epoch 42/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1042 - mae: 0.2494 - val_loss: 0.1055 - val_mae: 0.2516\n",
            "Epoch 43/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1039 - mae: 0.2493 - val_loss: 0.1063 - val_mae: 0.2517\n",
            "Epoch 44/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1033 - mae: 0.2482 - val_loss: 0.1052 - val_mae: 0.2504\n",
            "Epoch 45/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1008 - mae: 0.2455 - val_loss: 0.1049 - val_mae: 0.2505\n",
            "Epoch 46/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1033 - mae: 0.2481 - val_loss: 0.1048 - val_mae: 0.2500\n",
            "Epoch 47/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1036 - mae: 0.2489 - val_loss: 0.1050 - val_mae: 0.2501\n",
            "Epoch 48/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1010 - mae: 0.2451 - val_loss: 0.1054 - val_mae: 0.2525\n",
            "Epoch 49/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1025 - mae: 0.2472 - val_loss: 0.1045 - val_mae: 0.2503\n",
            "Epoch 50/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1014 - mae: 0.2460 - val_loss: 0.1047 - val_mae: 0.2504\n",
            "Epoch 51/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0997 - mae: 0.2440 - val_loss: 0.1048 - val_mae: 0.2516\n",
            "Epoch 52/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1006 - mae: 0.2458 - val_loss: 0.1045 - val_mae: 0.2502\n",
            "Epoch 53/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0994 - mae: 0.2440 - val_loss: 0.1052 - val_mae: 0.2506\n",
            "Epoch 54/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1009 - mae: 0.2457 - val_loss: 0.1045 - val_mae: 0.2504\n",
            "Epoch 55/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1000 - mae: 0.2447 - val_loss: 0.1048 - val_mae: 0.2505\n",
            "Epoch 56/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0990 - mae: 0.2431 - val_loss: 0.1040 - val_mae: 0.2502\n",
            "Epoch 57/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1011 - mae: 0.2457 - val_loss: 0.1057 - val_mae: 0.2517\n",
            "Epoch 58/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1003 - mae: 0.2454 - val_loss: 0.1043 - val_mae: 0.2502\n",
            "Epoch 59/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0996 - mae: 0.2450 - val_loss: 0.1049 - val_mae: 0.2504\n",
            "Epoch 60/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0992 - mae: 0.2436 - val_loss: 0.1046 - val_mae: 0.2509\n",
            "Epoch 61/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0992 - mae: 0.2441 - val_loss: 0.1043 - val_mae: 0.2505\n",
            "Epoch 62/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0978 - mae: 0.2422 - val_loss: 0.1048 - val_mae: 0.2501\n",
            "Epoch 63/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0981 - mae: 0.2426 - val_loss: 0.1062 - val_mae: 0.2512\n",
            "Epoch 64/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0987 - mae: 0.2429 - val_loss: 0.1063 - val_mae: 0.2519\n",
            "Epoch 65/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0984 - mae: 0.2426 - val_loss: 0.1044 - val_mae: 0.2503\n",
            "Epoch 66/100\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0986 - mae: 0.2426 - val_loss: 0.1061 - val_mae: 0.2515\n",
            "Training time: 70.89 seconds\n",
            "Validation Score: 9.11% (Poverty: 7.09%, Consumption: 27.34%)\n",
            "\n",
            "============================================================\n",
            "ALL MODELS TRAINED\n",
            "============================================================\n",
            "\n",
            "[Step 5/7] Optimizing ensemble...\n",
            "\n",
            "============================================================\n",
            "OPTIMIZING BLEND WEIGHTS FOR COMPETITION METRIC\n",
            "============================================================\n",
            "\n",
            "Individual Model Scores (per survey metric):\n",
            "----------------------------------------\n",
            "  XGBoost: 8.70%\n",
            "  LightGBM: 9.91%\n",
            "  CatBoost: 9.89%\n",
            "  NeuralNet: 9.11%\n",
            "\n",
            "Best single model: XGBoost (8.70%)\n",
            "\n",
            "OPTIMAL BLEND WEIGHTS:\n",
            "----------------------------------------\n",
            "  XGBoost: 100.0%\n",
            "\n",
            "BLENDED VALIDATION SCORE: 8.70%\n",
            "  Poverty MAPE:    7.08%\n",
            "  Consumption MAPE: 23.28%\n",
            "\n",
            "[Step 6/7] Calibrating predictions (per survey)...\n",
            "\n",
            "============================================================\n",
            "CALIBRATING PREDICTIONS (Per survey)\n",
            "============================================================\n",
            "\n",
            "Score Before calibration: 8.70%\n",
            "\n",
            "--- Global calibration ---\n",
            "Global params: scale=1.0850, shift=-0.7501\n",
            "Global calibration score: 3.55%\n",
            "\n",
            "--- Per survey calibration ---\n",
            "  Survey 100000: scale=1.0897, shift=-0.7587, score=3.30%\n",
            "  Survey 200000: scale=1.0914, shift=-0.7406, score=2.95%\n",
            "  Survey 300000: scale=1.0924, shift=-0.8620, score=2.93%\n",
            "\n",
            "Per survey calibration score: 3.06%\n",
            "\n",
            "✓ Using per survey calibration (better by 0.48%)\n",
            "\n",
            "Final improvement: 5.64 percentage points\n",
            "Score: 8.70% → 3.06%\n",
            "\n",
            "Generating test predictions for all models...\n",
            "Test predictions generated for all models\n",
            "\n",
            "Blending test predictions...\n",
            "\n",
            "Applied per survey calibration:\n",
            "  Survey 400000 (using 100000 params): scale=1.0897, shift=-0.7587\n",
            "  Survey 500000 (using 200000 params): scale=1.0914, shift=-0.7406\n",
            "  Survey 600000 (using 300000 params): scale=1.0924, shift=-0.8620\n",
            "\n",
            "Final Test Predictions (after per survey calibration):\n",
            "  Mean:   $11.98/day\n",
            "  Median: $9.60/day\n",
            "  Min:    $0.79/day\n",
            "  Max:    $126.25/day\n",
            "\n",
            "Per survey prediction statistics:\n",
            "  Survey 400000: Mean=$11.70, Median=$9.31\n",
            "  Survey 500000: Mean=$12.19, Median=$9.82\n",
            "  Survey 600000: Mean=$12.05, Median=$9.68\n",
            "\n",
            "[Step 7/7] Generating submission...\n",
            "\n",
            "============================================================\n",
            "GENERATING SUBMISSION FILES\n",
            "============================================================\n",
            "\n",
            "Files created:\n",
            "  1. predicted_household_consumption.csv - Shape: (103023, 3)\n",
            "  2. predicted_poverty_distribution.csv - Shape: (3, 20)\n",
            "  3. submission.zip\n",
            "\n",
            "Prediction Statistics:\n",
            "  Mean:   $11.98/day\n",
            "  Median: $9.60/day\n",
            "  Min:    $0.79/day\n",
            "  Max:    $126.25/day\n",
            "\n",
            "Predicted Poverty Rates (at $5.26/day threshold):\n",
            "  Survey 400000: 20.0%\n",
            "  Survey 500000: 16.8%\n",
            "  Survey 600000: 18.3%\n",
            "\n",
            "✓ Ready to submit to DrivenData!\n",
            "\n",
            "============================================================\n",
            "PIPELINE COMPLETE!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINAL SUMMARY\n",
            "============================================================\n",
            "Best single model: XGBoost (8.70%)\n",
            "Ensemble weights: XGBoost=100.0% \n",
            "Calibration type: Per survey\n",
            "  Survey 100000: scale=1.0897, shift=-0.7587\n",
            "  Survey 200000: scale=1.0914, shift=-0.7406\n",
            "  Survey 300000: scale=1.0924, shift=-0.8620\n",
            "\n",
            "Score before calibration: 8.70%\n",
            "Score after calibration: 3.06%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4735c242-a25e-472a-8f47-afeba021ab70\", \"submission.zip\", 2433673)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download started!\n"
          ]
        }
      ]
    }
  ]
}