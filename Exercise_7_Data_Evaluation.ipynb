{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPyOWcwi0Z6obTq690qAmy0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avionerman/machine_learning_2025/blob/main/Exercise_7_Data_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Βιβλιοθήκες"
      ],
      "metadata": {
        "id": "6Rf8vTnPOZN-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuQWPUzwN1vi",
        "outputId": "b233b322-0674-48fd-ae1b-6668bcc32d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# !pip install ydata-profiling --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# sklearn for preprocessing & metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# For Random Forest for feature importance, etc.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# PyTorch for GPU-based classifier\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ydata-profiling\n",
        "# from ydata_profiling import ProfileReport\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 1"
      ],
      "metadata": {
        "id": "82ASLdEoOpFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/bankloan.csv\"\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# df.shape\n",
        "#df.info()\n",
        "# display(df.describe().T)\n",
        "#categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "# print(categorical_columns.size)\n",
        "#numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "# print(numerical_columns)\n",
        "#nan_total_ratio = df.isna().mean().sort_values(ascending=False)\n",
        "# display(nan_total_ratio)\n",
        "\n",
        "#profile = ProfileReport(df, title=\"Bank loan dataset\", explorative=True)\n",
        "#profile.to_file('report.json')"
      ],
      "metadata": {
        "id": "0LMcnIYsOwNo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 2 - Basic with Mean imputer"
      ],
      "metadata": {
        "id": "AhIixjstjd6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PeaqxgttFXYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Θα αφαιρεσω features οπως τα IDs (member_id, raw_id, κλπ_ διοτι ειναι μοναδικα και δεν συμβαλουν καπως σε συμπερασματα ποιοτικα. Υπαρχει μεγαλο ρισκο για overfitting αν τα κρατησω.\n",
        "*   Μεσω του nan_total_ratio παρατηρησα οτι υπαρχουν κενα σε πολλα features και θα δημιουργησει προβληματα. Θα τα αντικαταστησω με τον μεσο ορο (mean) το οποιο βεβαια θα μου μειωσει το variance ειναι ενα ρισκο που θα παρω, καθως το μειωμενο variance < nan values). Για αυτο θα δοκιμασω να εισαγω και τον kNN imputator που θα εχει καλυτερα αποτελεσματα αλλα θα ειναι πιο αργος (στο δευτερο σκελος - πραγματικα πολυ πολυ αργος!!)\n",
        "*   Τελος θα μετατρεψω την ιδεα της λυσης απο πολλες κλασεις, σε δυο (binary) για να το κανω πιο απλο και ευκολα κατανοητο προς παραπανω αναλυση.\n",
        "\n"
      ],
      "metadata": {
        "id": "b0uODkSQkVPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# normal imputer (Mean)\n",
        "\n",
        "# 2.1 Μέσο, Μέγιστο, Ελάχιστο loan_amnt\n",
        "min_loan = df['loan_amnt'].min()\n",
        "max_loan = df['loan_amnt'].max()\n",
        "mean_loan = df['loan_amnt'].mean()\n",
        "\n",
        "print(f\"\\nLoan Amount - Min: {min_loan}, Max: {max_loan}, Mean: {mean_loan:.2f}\")\n",
        "\n",
        "# 2.2 Feature Selection/Cleaning\n",
        "cols_to_drop = ['id', 'member_id', 'Row ID', 'title']\n",
        "df_clean = df.drop(columns=cols_to_drop)\n",
        "\n",
        "# 2.3 handle missing values combined with encoding\n",
        "numerical_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
        "\n",
        "# imputation with the mean\n",
        "df_clean[numerical_cols] = df_clean[numerical_cols].fillna(df_clean[numerical_cols].mean())\n",
        "\n",
        "# we drop NaN values from categorical > replace them (due to too many NaNs)\n",
        "df_clean.dropna(subset=categorical_cols, inplace=True)\n",
        "\n",
        "def define_target(sub_grade):\n",
        "    if pd.isna(sub_grade): return 0\n",
        "    if sub_grade.startswith('A'): return 1\n",
        "    if sub_grade in ['B1', 'B2']: return 1\n",
        "    return 0\n",
        "\n",
        "df_clean['Target'] = df_clean['sub_grade'].apply(define_target)\n",
        "\n",
        "print(\"\\ntarget: \", df_clean['Target'].value_counts(normalize=True))\n",
        "\n",
        "# bins for the loan amount (looking for the prob. < 15%)\n",
        "df_clean['loan_bin'] = pd.cut(df_clean['loan_amnt'], bins=range(0, 40000, 5000))\n",
        "prob_per_bin = df_clean.groupby('loan_bin', observed=False)['Target'].mean()\n",
        "\n",
        "#print(\"\\naccepted prob per bin: \", prob_per_bin)\n",
        "for x in prob_per_bin.index.tolist():\n",
        "  print(prob_per_bin[x])\n",
        "\n",
        "# range of probs\n",
        "valid_bins = prob_per_bin[prob_per_bin > 0.15]\n",
        "print(\"\\nΕύρη με πιθανότητα > 15%:\", valid_bins.index.tolist())\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVjcWUdyBw7U",
        "outputId": "60d532d4-97db-4a4a-b717-b1d814f9faeb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loan Amount - Min: 1000, Max: 35000, Mean: 15257.97\n",
            "\n",
            "target:  Target\n",
            "0    0.708492\n",
            "1    0.291508\n",
            "Name: proportion, dtype: float64\n",
            "0.25518882251695857\n",
            "0.35874252938987283\n",
            "0.29317818316682503\n",
            "0.2851569588096534\n",
            "0.2881468966743731\n",
            "0.28300173991981237\n",
            "0.1279341776236186\n",
            "\n",
            "Εύρη με πιθανότητα > 15%: [Interval(0, 5000, closed='right'), Interval(5000, 10000, closed='right'), Interval(10000, 15000, closed='right'), Interval(15000, 20000, closed='right'), Interval(20000, 25000, closed='right'), Interval(25000, 30000, closed='right')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 2 - Alt. with kNN imputer"
      ],
      "metadata": {
        "id": "oHm_REPQFJ0q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-j6vJnJ2v1Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 2 - Simple imputer"
      ],
      "metadata": {
        "id": "JBobTBQXUCGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"loan_amnt\" not in df.columns:\n",
        "    raise ValueError(\"Column 'loan_amnt' not found in dataset!\")\n",
        "\n",
        "loan_stats = df[\"loan_amnt\"].describe()\n",
        "print(\"\\n[2.1] loan_amnt stats:\")\n",
        "print(loan_stats[[\"min\", \"mean\", \"max\"]])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.2  Ποιες μεταβλητές αφαιρούμε και ποιες κρατάμε για το μοντέλο\n",
        "#      (κρατάμε ένα καθαρό σύνολο features για classification)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Features που ΘΑ ΧΡΗΣΙΜΟΠΟΙΗΣΟΥΜΕ στο μοντέλο (numeric + categorical)\n",
        "numeric_features = [\n",
        "    \"loan_amnt\",\n",
        "    \"funded_amnt\",\n",
        "    \"annual_inc\",\n",
        "    \"dti\",\n",
        "    \"delinq_2yrs\",\n",
        "    \"inq_last_6mths\",\n",
        "    \"open_acc\",\n",
        "    \"pub_rec\",\n",
        "    \"revol_bal\",\n",
        "    \"revol_util\",\n",
        "    \"total_acc\",\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    \"term\",\n",
        "    \"home_ownership\",\n",
        "    \"verification_status\",\n",
        "    \"purpose\",\n",
        "    \"initial_list_status\",\n",
        "    \"application_type\",\n",
        "]\n",
        "\n",
        "# Στήλες που ΓΕΝΙΚΑ δε θα χρησιμοποιήσουμε ως features (για αναφορά)\n",
        "#columns_to_drop_example = [\n",
        "#    \"id\", \"member_id\", \"emp_title\", \"title\", \"loan_status\",\n",
        "#    \"grade\", \"sub_grade\",  # επειδή ορίζουμε από αυτά το target\n",
        "#    \"Unnamed: 0\", \"Row ID\", \"Unnamed: 50\"\n",
        "#]\n",
        "#print(\"\\n[2.2] Example columns to drop / not use as X:\", columns_to_drop_example)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.3 + 2.4  Ορισμός target (good_loan) και basic preprocessing\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Πρέπει να υπάρχει η στήλη sub_grade για να ορίσουμε το target\n",
        "if \"sub_grade\" not in df.columns:\n",
        "    raise ValueError(\"Column 'sub_grade' not found in dataset!\")\n",
        "\n",
        "# 2.4: Ορισμός των good grades (A1–A5, B1, B2)\n",
        "good_grades = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2']\n",
        "df[\"good_loan\"] = df[\"sub_grade\"].isin(good_grades).astype(int)\n",
        "\n",
        "print(\"\\n[2.4] Target 'good_loan' value counts:\")\n",
        "print(df[\"good_loan\"].value_counts())\n",
        "print(\"\\n[2.4] Target 'good_loan' distribution (ratio):\")\n",
        "print(df[\"good_loan\"].value_counts(normalize=True))\n",
        "\n",
        "# Δημιουργούμε df_model μόνο με τα features που θέλουμε + target\n",
        "all_feature_cols = numeric_features + categorical_features\n",
        "missing_features = [c for c in all_feature_cols if c not in df.columns]\n",
        "if missing_features:\n",
        "    raise ValueError(f\"Missing expected feature columns: {missing_features}\")\n",
        "\n",
        "df_model = df[all_feature_cols + [\"good_loan\"]].copy()\n",
        "\n",
        "X = df_model[all_feature_cols]\n",
        "y = df_model[\"good_loan\"]\n",
        "\n",
        "print(\"\\n[2.3] X shape:\", X.shape)\n",
        "print(\"[2.3] y positive ratio (good_loan=1):\", y.mean())\n",
        "\n",
        "# Preprocessing pipelines\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fit μόνο στο X (train/test split θα γίνει στο 3.x)\n",
        "X_processed_sample = preprocess.fit_transform(X)\n",
        "print(\"\\n[2.3] Preprocessing fitted on X. Processed shape (for all data):\", X_processed_sample.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.5  loan_amnt ranges με P(good_loan=1) >= 15%\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Ομαδοποίηση σε bins 5.000€\n",
        "bins = list(range(0, 40001, 5000))  # 0–5k, 5–10k, ..., 35–40k\n",
        "df[\"loan_bin_5k\"] = pd.cut(df[\"loan_amnt\"], bins=bins)\n",
        "\n",
        "prob_by_bin = df.groupby(\"loan_bin_5k\", observed=True)[\"good_loan\"].mean()\n",
        "count_by_bin = df.groupby(\"loan_bin_5k\", observed=True)[\"good_loan\"].count()\n",
        "\n",
        "result_25 = pd.DataFrame(\n",
        "    {\n",
        "        \"n_samples\": count_by_bin,\n",
        "        \"good_loan_prob\": prob_by_bin,\n",
        "    }\n",
        ").sort_index()\n",
        "\n",
        "print(\"\\n[2.5] P(good_loan=1) ανά εύρος loan_amnt (bins 5.000€):\")\n",
        "print(result_25)\n",
        "\n",
        "# Φιλτράρουμε μόνο τα bins με prob >= 0.15 (15%)\n",
        "valid_bins = result_25[result_25[\"good_loan_prob\"] >= 0.15]\n",
        "print(\"\\n[2.5] Εύρη loan_amnt με P(good_loan=1) >= 15%:\")\n",
        "print(valid_bins if not valid_bins.empty else \"Κανένα εύρος δεν ικανοποιεί το κριτήριο.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2llZiyldUIKR",
        "outputId": "e71e62e9-9917-42ae-f913-2d820719ea28"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2.1] loan_amnt stats:\n",
            "min      1000.00000\n",
            "mean    15257.96553\n",
            "max     35000.00000\n",
            "Name: loan_amnt, dtype: float64\n",
            "\n",
            "[2.4] Target 'good_loan' value counts:\n",
            "good_loan\n",
            "0    151709\n",
            "1     61290\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[2.4] Target 'good_loan' distribution (ratio):\n",
            "good_loan\n",
            "0    0.712252\n",
            "1    0.287748\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "[2.3] X shape: (212999, 17)\n",
            "[2.3] y positive ratio (good_loan=1): 0.28774782980201785\n",
            "\n",
            "[2.3] Preprocessing fitted on X. Processed shape (for all data): (212999, 37)\n",
            "\n",
            "[2.5] P(good_loan=1) ανά εύρος loan_amnt (bins 5.000€):\n",
            "                n_samples  good_loan_prob\n",
            "loan_bin_5k                              \n",
            "(0, 5000]           23591        0.248018\n",
            "(5000, 10000]       52221        0.352425\n",
            "(10000, 15000]      48125        0.290306\n",
            "(15000, 20000]      36573        0.282640\n",
            "(20000, 25000]      24675        0.283728\n",
            "(25000, 30000]      14418        0.279096\n",
            "(30000, 35000]      13396        0.127053\n",
            "\n",
            "[2.5] Εύρη loan_amnt με P(good_loan=1) >= 15%:\n",
            "                n_samples  good_loan_prob\n",
            "loan_bin_5k                              \n",
            "(0, 5000]           23591        0.248018\n",
            "(5000, 10000]       52221        0.352425\n",
            "(10000, 15000]      48125        0.290306\n",
            "(15000, 20000]      36573        0.282640\n",
            "(20000, 25000]      24675        0.283728\n",
            "(25000, 30000]      14418        0.279096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 3"
      ],
      "metadata": {
        "id": "vA-eatIIFYqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα πειραματηστω με το RF γιατι μπορει και παραγει πολλα πιθανα δεντρα, αρα πολλα πιθανα αποτελεσματα και εχει τεχνικες να αγνοει τα μη ρεαλιστικα/καλα \"κλαδια\" του. Ετσι θεωρω πως στο τελος, ο ΜΟ των απαντησεων των θετικων κλαδιων θα μας δωσει αυτο που επιθυμουμε (δε θα εχω overfitting, δε θα επηρεαστει το αποτελεσμα τοσο πολυ απο τους outliers)\n",
        "\n",
        "*   Αρχικα αφαιρεσα τα grade, sub_grade και int_rate. Το int_rate γτ αρχικα το βαζει η ιδια η τραπεζα και δε θελω να κλεψω τον εαυτο μου. Μετα εβγαλα τα grades γιατι αν ξερουμε ηδη τον βαθμο του καθε πελατη, τοτε ειναι biased το μοντελο.\n",
        "*   Πρεπει να κανω κανονικοποιηση στα δεδομενα γιατι δε γινεται για παραδειγμα ενα feature με παραδειγμα των 100,000 να ερθει σε σωστη αναλογια αντιστοιχα με ενα feature του οποιου ο αριθμος ειναι 50, 100 ή κατι πιο μικρο. Δε θα παω σε MinMax γιατι αν εχω καποιον μεγαλο outlier δε θα εχει το καλυτερο αποτελεσμα.\n",
        "*    Θα βαλω οπως σε ολες τις ασκησεις το stratify για να υποστηριξω την αδυναμη κλαση σε περιπτωση που θα εχουμε inbalanced set.\n",
        "*    Τελος, θα προσπαθησω να βγαλω το precision απο το μοντελο για να δω οταν το μοντελο εγκρινει καποιον ποσο συχνα εχει δικιο (ποιοτικο metric). Και το recall για να καταλαβω ποσους καταφερε να εντωπισει απο τον συνολικο κουβα (ισως ποσοτικο metric)."
      ],
      "metadata": {
        "id": "i9AwvRWnFhdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    X\n",
        "    y\n",
        "    preprocess\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\n",
        "        \"X, y, or preprocess not found. Please run the 2.x preprocessing cell first.\"\n",
        "    ) from e\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.1  Normalization is already defined inside 'preprocess'\n",
        "#      (StandardScaler for numeric, OneHotEncoder for categorical)\n",
        "# ------------------------------------------------------------\n",
        "# Nothing extra to do here in code – we just *use* 'preprocess' properly\n",
        "# by fitting it ONLY on the training data (see below).\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.2  Train/Test split (70/30, stratified)\n",
        "# ------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "#    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape[0])\n",
        "print(\"Test size :\", X_test.shape[0])\n",
        "print(\"Train positive ratio:\", y_train.mean())\n",
        "print(\"Test positive ratio :\", y_test.mean())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.3  Preprocess + convert to GPU tensors + MLP model\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Fit preprocessing ONLY on train\n",
        "X_train_proc = preprocess.fit_transform(X_train)\n",
        "X_test_proc = preprocess.transform(X_test)\n",
        "\n",
        "# Convert sparse matrices to dense if needed\n",
        "if hasattr(X_train_proc, \"toarray\"):\n",
        "    X_train_proc = X_train_proc.toarray()\n",
        "    X_test_proc = X_test_proc.toarray()\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nUsing device:\", device)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train_proc, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_proc, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Dataset & DataLoader\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "batch_size = 1024\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "print(\"Input dimension:\", input_dim)\n",
        "\n",
        "# Define MLP model\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLPClassifier(input_dim).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Training loop on GPU\n",
        "# ------------------------------------------------------------\n",
        "n_epochs = 8  # you can tune\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_dl:\n",
        "        yb = yb.unsqueeze(1)  # shape (batch, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_ds)\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} - loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.4  Evaluation: Accuracy, Precision, Recall, F1\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_probs = model(X_test_tensor).cpu().numpy().ravel()\n",
        "\n",
        "test_pred = (test_probs >= 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test, test_pred)\n",
        "prec = precision_score(y_test, test_pred, zero_division=0)\n",
        "rec = recall_score(y_test, test_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, test_pred, zero_division=0)\n",
        "\n",
        "print(\"\\n=== Test Metrics ===\")\n",
        "print(f\"Accuracy : {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall   : {rec:.4f}\")\n",
        "print(f\"F1-score : {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBUTl3r9FddW",
        "outputId": "1474cc9b-f460-4322-f82c-1156ff1d02b9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 149099\n",
            "Test size : 63900\n",
            "Train positive ratio: 0.28835203455422237\n",
            "Test positive ratio : 0.2863380281690141\n",
            "\n",
            "Using device: cuda\n",
            "Input dimension: 37\n",
            "\n",
            "Model architecture:\n",
            "MLPClassifier(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=37, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (8): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Epoch 1/8 - loss: 0.4895\n",
            "Epoch 2/8 - loss: 0.4076\n",
            "Epoch 3/8 - loss: 0.3940\n",
            "Epoch 4/8 - loss: 0.3888\n",
            "Epoch 5/8 - loss: 0.3863\n",
            "Epoch 6/8 - loss: 0.3850\n",
            "Epoch 7/8 - loss: 0.3832\n",
            "Epoch 8/8 - loss: 0.3822\n",
            "\n",
            "=== Test Metrics ===\n",
            "Accuracy : 0.8231\n",
            "Precision: 0.7314\n",
            "Recall   : 0.6040\n",
            "F1-score : 0.6617\n"
          ]
        }
      ]
    }
  ]
}