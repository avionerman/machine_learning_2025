{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9DNIR-nBKjTL",
        "K4BltatpMXMC",
        "7cWv-ZKSM19_",
        "FNzsfVRuVXOB",
        "fOQwjTRKWpxk",
        "O8jjjkrEXo_6",
        "YO1gF6XJZrY1",
        "_58BQPf9ZyNr",
        "DCARJ1i8Z1V0",
        "fSktRsu0ovkH",
        "cJB4pElncd94",
        "7JEcZwCIeR_5",
        "RqqcOz_PiEoY",
        "IrV486BUjaj9",
        "tjjlOnx2kjYs",
        "6yB97ShomVbC",
        "oR7y7wp9qcL0",
        "RslnFQc9b8A9",
        "a51O8T4Edhl7",
        "17skUfq5gbe2",
        "v-LNNG-AhRDu",
        "fXRNohFliKLz",
        "u6DZvEifl1tV",
        "iEQ9lv_pnkCq",
        "nsU865RVoejb",
        "5qY1Z3zYqhBX",
        "dk7SxZ7cqob0",
        "LNEQUHjgq_KP",
        "FuUUvFbtrNOR",
        "FRHXIqWWriNG"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYyFom1AflF1fAYhnl/A2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avionerman/machine_learning_2025/blob/main/competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "s5HFlabxoTnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVNLXkymoV35",
        "outputId": "20ebeb2c-dd05-4e84-ccc5-26f5a9d66ca1"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Config"
      ],
      "metadata": {
        "id": "QMsrL2VToZT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Config:\n",
        "    \"\"\"Central configuration for all parameters.\"\"\"\n",
        "\n",
        "    # File paths (UPDATE THESE TO YOUR PATHS)\n",
        "    TRAIN_FEATURES_PATH = '/content/train_hh_features.csv'\n",
        "    TRAIN_LABELS_PATH = '/content/train_hh_gt.csv'\n",
        "    TEST_FEATURES_PATH = '/content/test_hh_features.csv'\n",
        "\n",
        "    # Target column\n",
        "    TARGET_COL = 'cons_ppp17'\n",
        "\n",
        "    # ID columns\n",
        "    ID_COLS = ['survey_id', 'hhid']\n",
        "\n",
        "    # Random state for reproducibility\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "    # Validation split\n",
        "    VAL_SIZE = 0.2\n",
        "\n",
        "    # Poverty thresholds for competition metric\n",
        "    POVERTY_THRESHOLDS = [\n",
        "        3.17, 3.94, 4.60, 5.26, 5.88, 6.47, 7.06, 7.70,\n",
        "        8.40, 9.13, 9.87, 10.70, 11.62, 12.69, 14.03,\n",
        "        15.64, 17.76, 20.99, 27.37\n",
        "    ]\n",
        "\n",
        "    # Percentile ranks for weighting\n",
        "    PERCENTILE_RANKS = [\n",
        "        0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40,\n",
        "        0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80,\n",
        "        0.85, 0.90, 0.95\n",
        "    ]\n",
        "\n",
        "    # Test survey IDs\n",
        "    TEST_SURVEY_IDS = [400000, 500000, 600000]\n",
        "\n",
        "    # Binary mappings (COMPLETE)\n",
        "    BINARY_MAPPINGS = {\n",
        "        'Yes': 1, 'No': 0,\n",
        "        'Male': 1, 'Female': 0,\n",
        "        'Access': 1, 'No access': 0,\n",
        "        'Urban': 1, 'Rural': 0,\n",
        "        'Owner': 1, 'Renter': 0, 'Not owner': 0,\n",
        "        'Employed': 1, 'Not employed': 0,\n",
        "        1: 1, 0: 0,\n",
        "        1.0: 1, 0.0: 0\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "w-y9sMytocTP"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for Data process"
      ],
      "metadata": {
        "id": "iVgTAcz-orIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles all data loading, cleaning, and preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load training and test data.\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "\n",
        "        train_features = pd.read_csv(self.config.TRAIN_FEATURES_PATH)\n",
        "        train_labels = pd.read_csv(self.config.TRAIN_LABELS_PATH)\n",
        "        test_features = pd.read_csv(self.config.TEST_FEATURES_PATH)\n",
        "\n",
        "        train = train_features.merge(train_labels, on=self.config.ID_COLS)\n",
        "\n",
        "        print(f\"Training data shape: {train.shape}\")\n",
        "        print(f\"Test data shape: {test_features.shape}\")\n",
        "\n",
        "        return train, test_features\n",
        "\n",
        "    def handle_missing_values(self, train, test):\n",
        "        \"\"\"Fill missing values in both datasets.\"\"\"\n",
        "        print(\"Handling missing values...\")\n",
        "\n",
        "        train['sector1d'] = train['sector1d'].fillna('Not employed')\n",
        "        test['sector1d'] = test['sector1d'].fillna('Not employed')\n",
        "\n",
        "        mode_dweltyp = train['dweltyp'].mode()[0]\n",
        "        train['dweltyp'] = train['dweltyp'].fillna(mode_dweltyp)\n",
        "        test['dweltyp'] = test['dweltyp'].fillna(mode_dweltyp)\n",
        "\n",
        "        median_utl = train['utl_exp_ppp17'].median()\n",
        "        train['utl_exp_ppp17'] = train['utl_exp_ppp17'].fillna(median_utl)\n",
        "        test['utl_exp_ppp17'] = test['utl_exp_ppp17'].fillna(median_utl)\n",
        "\n",
        "        cols_to_fill = ['employed', 'share_secondary', 'educ_max']\n",
        "        for col in cols_to_fill:\n",
        "            if col in train.columns and train[col].isnull().sum() > 0:\n",
        "                mode_val = train[col].mode()[0]\n",
        "                train[col] = train[col].fillna(mode_val)\n",
        "                test[col] = test[col].fillna(mode_val)\n",
        "\n",
        "        consumed_cols = [col for col in train.columns if col.startswith('consumed')]\n",
        "        for col in consumed_cols:\n",
        "            if train[col].isnull().sum() > 0:\n",
        "                mode_val = train[col].mode()[0]\n",
        "                train[col] = train[col].fillna(mode_val)\n",
        "                test[col] = test[col].fillna(mode_val)\n",
        "\n",
        "        print(f\"Missing values after filling: {train.isnull().sum().sum()}\")\n",
        "        return train, test\n",
        "\n",
        "    def encode_binary_columns(self, train, test):\n",
        "        \"\"\"Encode binary columns with robust handling.\"\"\"\n",
        "        print(\"Encoding binary columns...\")\n",
        "\n",
        "        consumed_cols = [col for col in train.columns if col.startswith('consumed')]\n",
        "        binary_cols = ['male', 'owner', 'water', 'toilet', 'sewer', 'elect',\n",
        "                       'employed', 'any_nonagric', 'urban'] + consumed_cols\n",
        "\n",
        "        for col in binary_cols:\n",
        "            if col in train.columns:\n",
        "                unique_train = train[col].unique()\n",
        "                unique_test = test[col].unique()\n",
        "\n",
        "                if train[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
        "                    train[col] = train[col].astype(int)\n",
        "                    test[col] = test[col].astype(int)\n",
        "                else:\n",
        "                    train[col] = train[col].map(self.config.BINARY_MAPPINGS)\n",
        "                    test[col] = test[col].map(self.config.BINARY_MAPPINGS)\n",
        "\n",
        "                    if train[col].isnull().any():\n",
        "                        print(f\"  Warning: Unmapped values in {col} (train). Unique before: {unique_train}\")\n",
        "                        train[col] = train[col].fillna(0)\n",
        "                    if test[col].isnull().any():\n",
        "                        print(f\"  Warning: Unmapped values in {col} (test). Unique before: {unique_test}\")\n",
        "                        test[col] = test[col].fillna(0)\n",
        "\n",
        "                    train[col] = train[col].astype(int)\n",
        "                    test[col] = test[col].astype(int)\n",
        "\n",
        "        print(f\"Binary columns encoded: {len(binary_cols)}\")\n",
        "        return train, test, binary_cols\n",
        "\n",
        "    def encode_multiclass_columns(self, train, test):\n",
        "        \"\"\"Encode multiclass columns using LabelEncoder.\"\"\"\n",
        "        print(\"Encoding multiclass columns...\")\n",
        "\n",
        "        multiclass_cols = ['water_source', 'sanitation_source', 'dweltyp', 'educ_max', 'sector1d']\n",
        "\n",
        "        for col in multiclass_cols:\n",
        "            if col in train.columns:\n",
        "                le = LabelEncoder()\n",
        "                train[col] = train[col].astype(str)\n",
        "                test[col] = test[col].astype(str)\n",
        "\n",
        "                combined = pd.concat([train[col], test[col]], axis=0)\n",
        "                le.fit(combined)\n",
        "                train[col] = le.transform(train[col])\n",
        "                test[col] = le.transform(test[col])\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        print(f\"Multiclass columns encoded: {len(multiclass_cols)}\")\n",
        "        return train, test, multiclass_cols\n",
        "\n",
        "    def define_feature_columns(self, train, binary_cols, multiclass_cols):\n",
        "        \"\"\"Define all feature columns.\"\"\"\n",
        "        print(\"Defining feature columns...\")\n",
        "\n",
        "        numerical_cols = [\n",
        "            'weight', 'utl_exp_ppp17', 'hsize', 'num_children5', 'num_children10',\n",
        "            'num_children18', 'age', 'num_adult_female', 'num_adult_male',\n",
        "            'num_elderly', 'sworkershh', 'share_secondary', 'sfworkershh'\n",
        "        ]\n",
        "\n",
        "        region_cols = ['region1', 'region2', 'region3', 'region4', 'region5', 'region6', 'region7']\n",
        "\n",
        "        feature_cols = binary_cols + multiclass_cols + numerical_cols + region_cols\n",
        "        feature_cols = [col for col in feature_cols if col in train.columns and col != self.config.TARGET_COL]\n",
        "        feature_cols = list(dict.fromkeys(feature_cols))\n",
        "\n",
        "        self.feature_cols = feature_cols\n",
        "        print(f\"Total feature columns: {len(feature_cols)}\")\n",
        "\n",
        "        return feature_cols\n",
        "\n",
        "    def prepare_data(self, train, test, feature_cols):\n",
        "        \"\"\"Prepare feature matrices and target.\"\"\"\n",
        "        print(\"Preparing feature matrices...\")\n",
        "\n",
        "        X = train[feature_cols].copy()\n",
        "        y = train[self.config.TARGET_COL].copy()\n",
        "        X_test = test[feature_cols].copy()\n",
        "\n",
        "        sample_weights = train['weight'].values\n",
        "        survey_ids = train['survey_id'].values\n",
        "\n",
        "        y_log = np.log1p(y)\n",
        "\n",
        "        print(f\"X shape: {X.shape}\")\n",
        "        print(f\"y shape: {y.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "        return X, y_log, X_test, sample_weights, survey_ids\n",
        "\n",
        "    def create_splits(self, X, y_log, sample_weights, survey_ids):\n",
        "        \"\"\"Create train/validation splits with survey IDs.\"\"\"\n",
        "        print(\"Creating train/validation splits...\")\n",
        "\n",
        "        x_train, x_val, y_train, y_val, w_train, w_val, sid_train, sid_val = train_test_split(\n",
        "            X, y_log, sample_weights, survey_ids,\n",
        "            test_size=self.config.VAL_SIZE,\n",
        "            random_state=self.config.RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {x_train.shape[0]} samples\")\n",
        "        print(f\"Validation set: {x_val.shape[0]} samples\")\n",
        "        print(f\"Surveys in validation: {np.unique(sid_val)}\")\n",
        "\n",
        "        return x_train, x_val, y_train, y_val, w_train, w_val, sid_train, sid_val\n",
        "\n",
        "    def scale_features(self, x_train, x_val, X, X_test):\n",
        "        \"\"\"Scale features for neural network.\"\"\"\n",
        "        print(\"Scaling features...\")\n",
        "\n",
        "        x_train_scaled = self.scaler.fit_transform(x_train)\n",
        "        x_val_scaled = self.scaler.transform(x_val)\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        return x_train_scaled, x_val_scaled, X_scaled, X_test_scaled\n",
        "\n",
        "    def process_all(self):\n",
        "        \"\"\"Run the complete data processing pipeline.\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STARTING DATA PROCESSING PIPELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        train, test = self.load_data()\n",
        "        train, test = self.handle_missing_values(train, test)\n",
        "        train, test, binary_cols = self.encode_binary_columns(train, test)\n",
        "        train, test, multiclass_cols = self.encode_multiclass_columns(train, test)\n",
        "        feature_cols = self.define_feature_columns(train, binary_cols, multiclass_cols)\n",
        "        X, y_log, X_test, sample_weights, survey_ids = self.prepare_data(train, test, feature_cols)\n",
        "        x_train, x_val, y_train, y_val, w_train, w_val, sid_train, sid_val = self.create_splits(\n",
        "            X, y_log, sample_weights, survey_ids\n",
        "        )\n",
        "        x_train_scaled, x_val_scaled, X_scaled, X_test_scaled = self.scale_features(x_train, x_val, X, X_test)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"DATA PROCESSING COMPLETE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return {\n",
        "            'train': train,\n",
        "            'test': test,\n",
        "            'X': X,\n",
        "            'y_log': y_log,\n",
        "            'X_test': X_test,\n",
        "            'sample_weights': sample_weights,\n",
        "            'survey_ids': survey_ids,\n",
        "            'x_train': x_train,\n",
        "            'x_val': x_val,\n",
        "            'y_train': y_train,\n",
        "            'y_val': y_val,\n",
        "            'w_train': w_train,\n",
        "            'w_val': w_val,\n",
        "            'sid_train': sid_train,\n",
        "            'sid_val': sid_val,\n",
        "            'x_train_scaled': x_train_scaled,\n",
        "            'x_val_scaled': x_val_scaled,\n",
        "            'X_scaled': X_scaled,\n",
        "            'X_test_scaled': X_test_scaled,\n",
        "            'feature_cols': feature_cols\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "hb5njEF9ouED"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for Competition's Metric calc."
      ],
      "metadata": {
        "id": "I2s-ki86o0M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"\n",
        "    Calculates the competition metric exactly as defined:\n",
        "    metric = (1/S) × Σs [ (90/Σw) × Σt(wt × |rt - r̂t|/rt) + (10/H) × Σh(|ch - ĉh|/ch) ]\n",
        "\n",
        "    Key: Calculate per survey, then average across surveys.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.threshold_weights = [1 - abs(0.4 - p) for p in config.PERCENTILE_RANKS]\n",
        "        self.sum_weights = sum(self.threshold_weights)\n",
        "\n",
        "    def calculate(self, y_pred_log, y_true_log, survey_ids=None, return_details=False):\n",
        "        \"\"\"\n",
        "        Calculate competition metric for predictions in log scale.\n",
        "        \"\"\"\n",
        "        y_pred = np.expm1(y_pred_log)\n",
        "        y_true = np.expm1(y_true_log)\n",
        "\n",
        "        return self.calculate_from_original_scale(y_pred, y_true, survey_ids, return_details)\n",
        "\n",
        "    def calculate_from_original_scale(self, y_pred, y_true, survey_ids=None, return_details=False):\n",
        "        \"\"\"\n",
        "        Calculate competition metric when predictions are already in original scale.\n",
        "        \"\"\"\n",
        "        if survey_ids is None:\n",
        "            survey_ids = np.zeros(len(y_true), dtype=int)\n",
        "\n",
        "        unique_surveys = np.unique(survey_ids)\n",
        "\n",
        "        survey_scores = []\n",
        "        survey_poverty_mapes = []\n",
        "        survey_consumption_mapes = []\n",
        "\n",
        "        for survey_id in unique_surveys:\n",
        "            mask = survey_ids == survey_id\n",
        "            y_true_survey = y_true[mask]\n",
        "            y_pred_survey = y_pred[mask]\n",
        "\n",
        "            # Calculate weighted poverty rate MAPE for this survey\n",
        "            poverty_errors = []\n",
        "            for threshold, tw in zip(self.config.POVERTY_THRESHOLDS, self.threshold_weights):\n",
        "                actual_rate = (y_true_survey < threshold).mean()\n",
        "                pred_rate = (y_pred_survey < threshold).mean()\n",
        "                if actual_rate > 0:\n",
        "                    error = tw * abs(actual_rate - pred_rate) / actual_rate\n",
        "                    poverty_errors.append(error)\n",
        "\n",
        "            poverty_mape = sum(poverty_errors) / self.sum_weights if poverty_errors else 0\n",
        "\n",
        "            # Calculate consumption MAPE for this survey\n",
        "            consumption_mape = np.mean(np.abs(y_true_survey - y_pred_survey) / y_true_survey)\n",
        "\n",
        "            # Survey score: 90% poverty + 10% consumption\n",
        "            survey_score = 0.9 * poverty_mape + 0.1 * consumption_mape\n",
        "\n",
        "            survey_scores.append(survey_score)\n",
        "            survey_poverty_mapes.append(poverty_mape)\n",
        "            survey_consumption_mapes.append(consumption_mape)\n",
        "\n",
        "        final_score = np.mean(survey_scores)\n",
        "        avg_poverty_mape = np.mean(survey_poverty_mapes)\n",
        "        avg_consumption_mape = np.mean(survey_consumption_mapes)\n",
        "\n",
        "        if return_details:\n",
        "            return final_score, avg_poverty_mape, avg_consumption_mape\n",
        "        return final_score\n",
        "\n"
      ],
      "metadata": {
        "id": "AMX1ctB_o31v"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for Models training"
      ],
      "metadata": {
        "id": "kRkc4baho90g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Trains XGBoost model - our best performer.\"\"\"\n",
        "\n",
        "    def __init__(self, config, metric):\n",
        "        self.config = config\n",
        "        self.metric = metric\n",
        "        self.model = None\n",
        "        self.val_predictions = None\n",
        "        self.test_predictions = None\n",
        "\n",
        "    def train_xgboost(self, X, y_log, x_val, y_val, sid_val):\n",
        "        \"\"\"Train XGBoost model.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Training XGBoost...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        model = xgb.XGBRegressor(\n",
        "            n_estimators=300,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.7,\n",
        "            colsample_bytree=0.8,\n",
        "            min_child_weight=3,\n",
        "            random_state=self.config.RANDOM_STATE,\n",
        "            tree_method='hist'\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y_log)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Training time: {train_time:.2f} seconds\")\n",
        "\n",
        "        # Get validation predictions\n",
        "        y_pred_val = model.predict(x_val)\n",
        "\n",
        "        # Calculate score BEFORE calibration\n",
        "        score, pov_mape, cons_mape = self.metric.calculate(\n",
        "            y_pred_val, y_val.values, sid_val, return_details=True\n",
        "        )\n",
        "        print(f\"Validation Score (before calibration): {score*100:.2f}%\")\n",
        "        print(f\"  Poverty MAPE:    {pov_mape*100:.2f}%\")\n",
        "        print(f\"  Consumption MAPE: {cons_mape*100:.2f}%\")\n",
        "\n",
        "        self.model = model\n",
        "        self.val_predictions = y_pred_val\n",
        "\n",
        "        return model, y_pred_val\n",
        "\n",
        "    def generate_test_predictions(self, X_test):\n",
        "        \"\"\"Generate predictions on test set.\"\"\"\n",
        "        print(\"\\nGenerating test predictions...\")\n",
        "        self.test_predictions = self.model.predict(X_test)\n",
        "        print(f\"Test predictions generated: {len(self.test_predictions)}\")\n",
        "        return self.test_predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "sB1XX-UZo_qH"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Predictions"
      ],
      "metadata": {
        "id": "wnxNunQppGPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PredictionCalibrator:\n",
        "    \"\"\"\n",
        "    Post-processing calibration to directly optimize the competition metric.\n",
        "\n",
        "    The key insight: Our model minimizes RMSE, but the competition scores on\n",
        "    poverty rate MAPE. By calibrating predictions with y_cal = scale * y + shift,\n",
        "    we can directly optimize what the competition measures.\n",
        "\n",
        "    This is a standard technique used by competition winners.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, metric):\n",
        "        self.config = config\n",
        "        self.metric = metric\n",
        "        self.best_params = None\n",
        "        self.best_score = None\n",
        "\n",
        "    def calibrate(self, y_pred_log, y_true_log, survey_ids):\n",
        "        \"\"\"\n",
        "        Find optimal calibration parameters (scale, shift) that minimize\n",
        "        the competition metric.\n",
        "\n",
        "        Args:\n",
        "            y_pred_log: Raw predictions in log scale\n",
        "            y_true_log: True values in log scale\n",
        "            survey_ids: Survey IDs for each sample\n",
        "\n",
        "        Returns:\n",
        "            Calibrated predictions in log scale\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"CALIBRATING PREDICTIONS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Convert to original scale for calibration\n",
        "        y_pred_orig = np.expm1(y_pred_log)\n",
        "        y_true_orig = np.expm1(y_true_log)\n",
        "\n",
        "        # Score before calibration\n",
        "        score_before = self.metric.calculate_from_original_scale(\n",
        "            y_pred_orig, y_true_orig, survey_ids\n",
        "        )\n",
        "        print(f\"\\nScore BEFORE calibration: {score_before*100:.2f}%\")\n",
        "\n",
        "        # Define objective function to minimize\n",
        "        def objective(params):\n",
        "            scale, shift = params\n",
        "            y_calibrated = scale * y_pred_orig + shift\n",
        "            # Ensure no negative values\n",
        "            y_calibrated = np.maximum(y_calibrated, 0.01)\n",
        "            return self.metric.calculate_from_original_scale(\n",
        "                y_calibrated, y_true_orig, survey_ids\n",
        "            )\n",
        "\n",
        "        # Try multiple starting points to avoid local minima\n",
        "        best_result = None\n",
        "        best_score = float('inf')\n",
        "\n",
        "        starting_points = [\n",
        "            [1.0, 0.0],      # No change\n",
        "            [1.0, 0.5],      # Shift up\n",
        "            [1.0, -0.5],     # Shift down\n",
        "            [1.05, 0.0],     # Scale up\n",
        "            [0.95, 0.0],     # Scale down\n",
        "            [1.02, 0.2],     # Scale up + shift up\n",
        "            [0.98, -0.2],    # Scale down + shift down\n",
        "            [1.1, 0.0],      # Larger scale up\n",
        "            [0.9, 0.0],      # Larger scale down\n",
        "        ]\n",
        "\n",
        "        print(\"\\nSearching for optimal calibration parameters...\")\n",
        "\n",
        "        for start in starting_points:\n",
        "            result = minimize(\n",
        "                objective,\n",
        "                start,\n",
        "                method='Nelder-Mead',\n",
        "                options={'maxiter': 1000, 'xatol': 1e-6, 'fatol': 1e-6}\n",
        "            )\n",
        "\n",
        "            if result.fun < best_score:\n",
        "                best_score = result.fun\n",
        "                best_result = result\n",
        "\n",
        "        # Also try bounded optimization\n",
        "        result_bounded = minimize(\n",
        "            objective,\n",
        "            [1.0, 0.0],\n",
        "            method='L-BFGS-B',\n",
        "            bounds=[(0.8, 1.2), (-2.0, 2.0)],\n",
        "            options={'maxiter': 1000}\n",
        "        )\n",
        "\n",
        "        if result_bounded.fun < best_score:\n",
        "            best_score = result_bounded.fun\n",
        "            best_result = result_bounded\n",
        "\n",
        "        self.best_params = best_result.x\n",
        "        self.best_score = best_score\n",
        "\n",
        "        scale, shift = self.best_params\n",
        "\n",
        "        print(f\"\\nOptimal calibration parameters:\")\n",
        "        print(f\"  Scale: {scale:.4f}\")\n",
        "        print(f\"  Shift: {shift:.4f}\")\n",
        "        print(f\"\\nScore AFTER calibration: {best_score*100:.2f}%\")\n",
        "        print(f\"Improvement: {(score_before - best_score)*100:.2f} percentage points\")\n",
        "\n",
        "        # Apply calibration\n",
        "        y_calibrated_orig = scale * y_pred_orig + shift\n",
        "        y_calibrated_orig = np.maximum(y_calibrated_orig, 0.01)\n",
        "\n",
        "        # Convert back to log scale\n",
        "        y_calibrated_log = np.log1p(y_calibrated_orig)\n",
        "\n",
        "        # Verify improvement per survey\n",
        "        print(\"\\nPer-survey improvement:\")\n",
        "        for sid in np.unique(survey_ids):\n",
        "            mask = survey_ids == sid\n",
        "            score_before_s = self.metric.calculate_from_original_scale(\n",
        "                y_pred_orig[mask], y_true_orig[mask], None\n",
        "            )\n",
        "            score_after_s = self.metric.calculate_from_original_scale(\n",
        "                y_calibrated_orig[mask], y_true_orig[mask], None\n",
        "            )\n",
        "            print(f\"  Survey {sid}: {score_before_s*100:.2f}% → {score_after_s*100:.2f}%\")\n",
        "\n",
        "        return y_calibrated_log\n",
        "\n",
        "    def apply_calibration(self, y_pred_log):\n",
        "        \"\"\"\n",
        "        Apply the learned calibration parameters to new predictions.\n",
        "\n",
        "        Args:\n",
        "            y_pred_log: Raw predictions in log scale\n",
        "\n",
        "        Returns:\n",
        "            Calibrated predictions in log scale\n",
        "        \"\"\"\n",
        "        if self.best_params is None:\n",
        "            raise ValueError(\"Must call calibrate() first to learn parameters\")\n",
        "\n",
        "        scale, shift = self.best_params\n",
        "\n",
        "        # Convert to original scale\n",
        "        y_pred_orig = np.expm1(y_pred_log)\n",
        "\n",
        "        # Apply calibration\n",
        "        y_calibrated_orig = scale * y_pred_orig + shift\n",
        "        y_calibrated_orig = np.maximum(y_calibrated_orig, 0.01)\n",
        "\n",
        "        # Convert back to log scale\n",
        "        y_calibrated_log = np.log1p(y_calibrated_orig)\n",
        "\n",
        "        return y_calibrated_log\n",
        "\n"
      ],
      "metadata": {
        "id": "hNVUH1gspH9W"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Submission"
      ],
      "metadata": {
        "id": "Wl-8zyjipLsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SubmissionGenerator:\n",
        "    \"\"\"Generates submission files for DrivenData competition.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def generate(self, test_features, y_test_pred):\n",
        "        \"\"\"Generate submission files.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"GENERATING SUBMISSION FILES\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Household consumption predictions\n",
        "        submission_hh = pd.DataFrame({\n",
        "            'survey_id': test_features['survey_id'],\n",
        "            'hhid': test_features['hhid'],\n",
        "            'cons_ppp17': y_test_pred\n",
        "        })\n",
        "\n",
        "        # 2. Poverty rates predictions\n",
        "        poverty_rates = []\n",
        "        for survey_id in self.config.TEST_SURVEY_IDS:\n",
        "            survey_preds = submission_hh[submission_hh['survey_id'] == survey_id]['cons_ppp17']\n",
        "            row = {'survey_id': survey_id}\n",
        "            for threshold in self.config.POVERTY_THRESHOLDS:\n",
        "                col_name = f'pct_hh_below_{threshold:.2f}'\n",
        "                pct_below = (survey_preds < threshold).mean()\n",
        "                row[col_name] = pct_below\n",
        "            poverty_rates.append(row)\n",
        "\n",
        "        submission_rates = pd.DataFrame(poverty_rates)\n",
        "\n",
        "        # Save files\n",
        "        submission_hh.to_csv('predicted_household_consumption.csv', index=False)\n",
        "        submission_rates.to_csv('predicted_poverty_distribution.csv', index=False)\n",
        "\n",
        "        # Create zip file\n",
        "        with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
        "            zipf.write('predicted_household_consumption.csv')\n",
        "            zipf.write('predicted_poverty_distribution.csv')\n",
        "\n",
        "        print(\"\\nFiles created:\")\n",
        "        print(f\"  1. predicted_household_consumption.csv - Shape: {submission_hh.shape}\")\n",
        "        print(f\"  2. predicted_poverty_distribution.csv - Shape: {submission_rates.shape}\")\n",
        "        print(f\"  3. submission.zip\")\n",
        "\n",
        "        # Show prediction statistics\n",
        "        print(f\"\\nPrediction Statistics:\")\n",
        "        print(f\"  Mean:   ${y_test_pred.mean():.2f}/day\")\n",
        "        print(f\"  Median: ${np.median(y_test_pred):.2f}/day\")\n",
        "        print(f\"  Min:    ${y_test_pred.min():.2f}/day\")\n",
        "        print(f\"  Max:    ${y_test_pred.max():.2f}/day\")\n",
        "\n",
        "        # Show poverty rates\n",
        "        print(f\"\\nPredicted Poverty Rates (at $5.26/day threshold):\")\n",
        "        for _, row in submission_rates.iterrows():\n",
        "            print(f\"  Survey {int(row['survey_id'])}: {row['pct_hh_below_5.26']*100:.1f}%\")\n",
        "\n",
        "        print(\"\\n✓ Ready to submit to DrivenData!\")\n",
        "\n",
        "        return submission_hh, submission_rates\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the submission zip file.\"\"\"\n",
        "        from google.colab import files\n",
        "        files.download('submission.zip')\n",
        "        print(\"Download started!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "O0bVR_ldpNRB"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "KtuDJrz1pP3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main pipeline with post-processing calibration:\n",
        "    1. Process data\n",
        "    2. Train XGBoost model\n",
        "    3. Calibrate predictions to optimize competition metric\n",
        "    4. Apply calibration to test predictions\n",
        "    5. Generate submission\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"POVERTY PREDICTION PIPELINE\")\n",
        "    print(\"With Post-Processing Calibration\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Initialize configuration\n",
        "    print(\"\\n[Step 1/6] Initializing configuration...\")\n",
        "    config = Config()\n",
        "\n",
        "    # Step 2: Process data\n",
        "    print(\"\\n[Step 2/6] Processing data...\")\n",
        "    processor = DataProcessor(config)\n",
        "    data = processor.process_all()\n",
        "\n",
        "    # Step 3: Initialize metric calculator\n",
        "    print(\"\\n[Step 3/6] Initializing competition metric...\")\n",
        "    metric = CompetitionMetric(config)\n",
        "\n",
        "    # Step 4: Train XGBoost model\n",
        "    print(\"\\n[Step 4/6] Training XGBoost model...\")\n",
        "    trainer = ModelTrainer(config, metric)\n",
        "    model, val_predictions = trainer.train_xgboost(\n",
        "        data['X'], data['y_log'],\n",
        "        data['x_val'], data['y_val'], data['sid_val']\n",
        "    )\n",
        "\n",
        "    # Step 5: Calibrate predictions\n",
        "    print(\"\\n[Step 5/6] Calibrating predictions...\")\n",
        "    calibrator = PredictionCalibrator(config, metric)\n",
        "\n",
        "    # Learn calibration parameters on validation set\n",
        "    val_predictions_calibrated = calibrator.calibrate(\n",
        "        val_predictions,\n",
        "        data['y_val'].values,\n",
        "        data['sid_val']\n",
        "    )\n",
        "\n",
        "    # Generate test predictions and apply calibration\n",
        "    test_predictions_raw = trainer.generate_test_predictions(data['X_test'])\n",
        "    test_predictions_calibrated = calibrator.apply_calibration(test_predictions_raw)\n",
        "\n",
        "    # Convert to original scale\n",
        "    y_test_pred = np.expm1(test_predictions_calibrated)\n",
        "\n",
        "    print(f\"\\nCalibrated Test Predictions:\")\n",
        "    print(f\"  Mean:   ${y_test_pred.mean():.2f}/day\")\n",
        "    print(f\"  Median: ${np.median(y_test_pred):.2f}/day\")\n",
        "    print(f\"  Min:    ${y_test_pred.min():.2f}/day\")\n",
        "    print(f\"  Max:    ${y_test_pred.max():.2f}/day\")\n",
        "\n",
        "    # Step 6: Generate submission\n",
        "    print(\"\\n[Step 6/6] Generating submission...\")\n",
        "    submission = SubmissionGenerator(config)\n",
        "    submission_hh, submission_rates = submission.generate(data['test'], y_test_pred)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'data': data,\n",
        "        'metric': metric,\n",
        "        'trainer': trainer,\n",
        "        'calibrator': calibrator,\n",
        "        'submission': submission,\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'calibration_params': calibrator.best_params\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "yOr5l3u9pRHB"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runner"
      ],
      "metadata": {
        "id": "iyWvBCMRpTOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the complete pipeline with calibration\n",
        "results = main()\n",
        "\n",
        "# Show calibration parameters used\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CALIBRATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Scale: {results['calibration_params'][0]:.4f}\")\n",
        "print(f\"Shift: {results['calibration_params'][1]:.4f}\")\n",
        "print(f\"Formula: y_calibrated = {results['calibration_params'][0]:.4f} × y_pred + {results['calibration_params'][1]:.4f}\")\n",
        "\n",
        "# Download submission file\n",
        "results['submission'].download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K312anyspVnu",
        "outputId": "db95fa4a-85c3-4a6c-cafa-d9b0f6504a2b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "POVERTY PREDICTION PIPELINE\n",
            "With Post-Processing Calibration\n",
            "============================================================\n",
            "\n",
            "[Step 1/6] Initializing configuration...\n",
            "\n",
            "[Step 2/6] Processing data...\n",
            "============================================================\n",
            "STARTING DATA PROCESSING PIPELINE\n",
            "============================================================\n",
            "Loading data...\n",
            "Training data shape: (104234, 89)\n",
            "Test data shape: (103023, 88)\n",
            "Handling missing values...\n",
            "Missing values after filling: 0\n",
            "Encoding binary columns...\n",
            "Binary columns encoded: 59\n",
            "Encoding multiclass columns...\n",
            "Multiclass columns encoded: 5\n",
            "Defining feature columns...\n",
            "Total feature columns: 84\n",
            "Preparing feature matrices...\n",
            "X shape: (104234, 84)\n",
            "y shape: (104234,)\n",
            "X_test shape: (103023, 84)\n",
            "Creating train/validation splits...\n",
            "Training set: 83387 samples\n",
            "Validation set: 20847 samples\n",
            "Surveys in validation: [100000 200000 300000]\n",
            "Scaling features...\n",
            "\n",
            "============================================================\n",
            "DATA PROCESSING COMPLETE\n",
            "============================================================\n",
            "\n",
            "[Step 3/6] Initializing competition metric...\n",
            "\n",
            "[Step 4/6] Training XGBoost model...\n",
            "\n",
            "============================================================\n",
            "Training XGBoost...\n",
            "============================================================\n",
            "Training time: 2.74 seconds\n",
            "Validation Score (before calibration): 8.70%\n",
            "  Poverty MAPE:    7.08%\n",
            "  Consumption MAPE: 23.28%\n",
            "\n",
            "[Step 5/6] Calibrating predictions...\n",
            "\n",
            "============================================================\n",
            "CALIBRATING PREDICTIONS\n",
            "============================================================\n",
            "\n",
            "Score BEFORE calibration: 8.70%\n",
            "\n",
            "Searching for optimal calibration parameters...\n",
            "\n",
            "Optimal calibration parameters:\n",
            "  Scale: 1.0850\n",
            "  Shift: -0.7501\n",
            "\n",
            "Score AFTER calibration: 3.55%\n",
            "Improvement: 5.15 percentage points\n",
            "\n",
            "Per-survey improvement:\n",
            "  Survey 100000: 8.03% → 3.36%\n",
            "  Survey 200000: 8.03% → 3.32%\n",
            "  Survey 300000: 10.04% → 3.96%\n",
            "\n",
            "Generating test predictions...\n",
            "Test predictions generated: 103023\n",
            "\n",
            "Calibrated Test Predictions:\n",
            "  Mean:   $11.94/day\n",
            "  Median: $9.58/day\n",
            "  Min:    $0.80/day\n",
            "  Max:    $125.50/day\n",
            "\n",
            "[Step 6/6] Generating submission...\n",
            "\n",
            "============================================================\n",
            "GENERATING SUBMISSION FILES\n",
            "============================================================\n",
            "\n",
            "Files created:\n",
            "  1. predicted_household_consumption.csv - Shape: (103023, 3)\n",
            "  2. predicted_poverty_distribution.csv - Shape: (3, 20)\n",
            "  3. submission.zip\n",
            "\n",
            "Prediction Statistics:\n",
            "  Mean:   $11.94/day\n",
            "  Median: $9.58/day\n",
            "  Min:    $0.80/day\n",
            "  Max:    $125.50/day\n",
            "\n",
            "Predicted Poverty Rates (at $5.26/day threshold):\n",
            "  Survey 400000: 20.1%\n",
            "  Survey 500000: 17.2%\n",
            "  Survey 600000: 17.8%\n",
            "\n",
            "✓ Ready to submit to DrivenData!\n",
            "\n",
            "============================================================\n",
            "PIPELINE COMPLETE!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "CALIBRATION SUMMARY\n",
            "============================================================\n",
            "Scale: 1.0850\n",
            "Shift: -0.7501\n",
            "Formula: y_calibrated = 1.0850 × y_pred + -0.7501\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d8b1e14-3ce0-4d1d-b0cf-dfb9093d0f95\", \"submission.zip\", 3334121)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download started!\n"
          ]
        }
      ]
    }
  ]
}